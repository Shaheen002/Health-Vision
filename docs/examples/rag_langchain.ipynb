{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaheen002/Health-Vision/blob/main/docs/examples/rag_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0Qh-aq8Zp9m"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docling-project/docling/blob/main/docs/examples/rag_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxLO80rqZp90"
      },
      "source": [
        "# RAG with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k_3g8HTZp93"
      },
      "source": [
        "| Step | Tech | Execution |\n",
        "| --- | --- | --- |\n",
        "| Embedding | Hugging Face / Sentence Transformers | ðŸ’» Local |\n",
        "| Vector store | Milvus | ðŸ’» Local |\n",
        "| Gen AI | Hugging Face Inference API | ðŸŒ Remote |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAOzX6QtZp94"
      },
      "source": [
        "This example leverages the\n",
        "[LangChain Docling integration](../../integrations/langchain/), along with a Milvus\n",
        "vector store, as well as sentence-transformers embeddings.\n",
        "\n",
        "The presented `DoclingLoader` component enables you to:\n",
        "- use various document types in your LLM applications with ease and speed, and\n",
        "- leverage Docling's rich format for advanced, document-native grounding.\n",
        "\n",
        "`DoclingLoader` supports two different export modes:\n",
        "- `ExportType.MARKDOWN`: if you want to capture each input document as a separate\n",
        "  LangChain document, or\n",
        "- `ExportType.DOC_CHUNKS` (default): if you want to have each input document chunked and\n",
        "  to then capture each individual chunk as a separate LangChain document downstream.\n",
        "\n",
        "The example allows exploring both modes via parameter `EXPORT_TYPE`; depending on the\n",
        "value set, the example pipeline is then set up accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDW_QSHtZp97"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvsprOBiZp-B"
      },
      "source": [
        "- ðŸ‘‰ For best conversion speed, use GPU acceleration whenever available; e.g. if running on Colab, use GPU-enabled runtime.\n",
        "- Notebook uses HuggingFace's Inference API; for increased LLM quota, token can be provided via env var `HF_TOKEN`.\n",
        "- Requirements can be installed as shown below (`--no-warn-conflicts` meant for Colab's pre-populated Python env; feel free to remove for stricter usage):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYBPdL_VZp-C",
        "outputId": "414180ce-b151-4b1c-be40-797043535227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q --progress-bar off --no-warn-conflicts langchain-docling langchain-core langchain-huggingface langchain_milvus langchain python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DOC_SOURCE = \"https://arxiv.org/pdf/2303.08518v4\""
      ],
      "metadata": {
        "id": "yQCsgSg7cFkh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "doc = DocumentConverter().convert(source=DOC_SOURCE).document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7BW1saLcM87",
        "outputId": "b039f62f-4496-4f97-f627-006dbbcb4f5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:docling.models.factories.base_factory:The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "WARNING:docling.models.factories.base_factory:The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:47,471 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:47,476 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:47,480 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/det/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:48,612 [RapidOCR] download_file.py:82: Download size: 13.83MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:48,816 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:48,817 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:49,293 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:49,294 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:49,296 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/cls/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:50,146 [RapidOCR] download_file.py:82: Download size: 0.56MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:50,171 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:50,172 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:50,253 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:50,253 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:50,255 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/rec/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:51,142 [RapidOCR] download_file.py:82: Download size: 25.67MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:51,589 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 06:19:51,590 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "WARNING:docling.models.factories.base_factory:The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "WARNING:docling.models.factories.base_factory:The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Docling Formula Text Mapper\n",
        "\n",
        "This module handles the extraction of formula content from the 'orig' field\n",
        "and copies it to the 'text' field in FormulaItem objects from Docling's\n",
        "document converter output.\n",
        "\n",
        "Logic:\n",
        "1. Parse the extracted Docling document schema\n",
        "2. Iterate through all text items in the body\n",
        "3. When a FormulaItem is found with 'orig' field populated but 'text' empty\n",
        "4. Copy the content from 'orig' to 'text' field\n",
        "5. Return the modified schema\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any, List, Optional\n",
        "import copy\n",
        "\n",
        "\n",
        "class DoclingFormulaMapper:\n",
        "    \"\"\"\n",
        "    Maps formula content from 'orig' field to 'text' field in Docling documents.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.formulas_processed = 0\n",
        "        self.formulas_skipped = 0\n",
        "\n",
        "    def process_formula_item(self, formula_item: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a single formula item and copy orig to text if needed.\n",
        "\n",
        "        Args:\n",
        "            formula_item: Dictionary containing formula item data\n",
        "\n",
        "        Returns:\n",
        "            Modified formula item with text field populated\n",
        "        \"\"\"\n",
        "        # Check if this is a FormulaItem\n",
        "        if not isinstance(formula_item, dict):\n",
        "            return formula_item\n",
        "\n",
        "        # Check if it has the label indicating it's a formula\n",
        "        if formula_item.get(\"label\") != \"formula\":\n",
        "            return formula_item\n",
        "\n",
        "        # Get orig and text fields\n",
        "        orig_content = formula_item.get(\"orig\", \"\")\n",
        "        text_content = formula_item.get(\"text\", \"\")\n",
        "\n",
        "        # If orig has content but text is empty, copy orig to text\n",
        "        if orig_content and not text_content:\n",
        "            formula_item[\"text\"] = orig_content\n",
        "            self.formulas_processed += 1\n",
        "\n",
        "            return formula_item\n",
        "        elif orig_content and text_content:\n",
        "            # Already has text, skip\n",
        "            self.formulas_skipped += 1\n",
        "            return formula_item\n",
        "        else:\n",
        "            # No orig content found\n",
        "            self.formulas_skipped += 1\n",
        "            return formula_item\n",
        "\n",
        "    def process_texts_list(self, texts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Process all text items and map formulas.\n",
        "\n",
        "        Args:\n",
        "            texts: List of text item dictionaries from Docling schema\n",
        "\n",
        "        Returns:\n",
        "            List of processed text items with formulas mapped\n",
        "        \"\"\"\n",
        "        processed_texts = []\n",
        "\n",
        "        for text_item in texts:\n",
        "            processed_item = self.process_formula_item(text_item)\n",
        "            processed_texts.append(processed_item)\n",
        "\n",
        "        return processed_texts\n",
        "\n",
        "    def process_document(self, docling_document_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process the entire Docling document and map all formulas.\n",
        "\n",
        "        Args:\n",
        "            docling_document_dict: Dictionary representation of the Docling document\n",
        "\n",
        "        Returns:\n",
        "            Modified document dictionary with all formulas mapped\n",
        "        \"\"\"\n",
        "        # Create a deep copy to avoid modifying the original\n",
        "        document = copy.deepcopy(docling_document_dict)\n",
        "\n",
        "        # Extract texts from the body\n",
        "        if \"texts\" in document and isinstance(document[\"texts\"], list):\n",
        "            document[\"texts\"] = self.process_texts_list(document[\"texts\"])\n",
        "\n",
        "        return document\n",
        "\n",
        "    def get_statistics(self) -> Dict[str, int]:\n",
        "        \"\"\"\n",
        "        Get statistics about the formula processing.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with processing statistics\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"formulas_processed\": self.formulas_processed,\n",
        "            \"formulas_skipped\": self.formulas_skipped,\n",
        "            \"total_processed\": self.formulas_processed + self.formulas_skipped,\n",
        "        }\n",
        "\n",
        "\n",
        "def extract_and_map_formulas(doc_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Standalone function to extract and map formulas in a document.\n",
        "\n",
        "    Args:\n",
        "        doc_dict: Dictionary representation of the Docling document\n",
        "\n",
        "    Returns:\n",
        "        Modified document with formulas mapped\n",
        "    \"\"\"\n",
        "    mapper = DoclingFormulaMapper()\n",
        "    processed_doc = mapper.process_document(doc_dict)\n",
        "\n",
        "    print(\"\\n=== Formula Mapping Statistics ===\")\n",
        "    stats = mapper.get_statistics()\n",
        "    for key, value in stats.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    return processed_doc\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from docling.document_converter import DocumentConverter\n",
        "\n",
        "    # Example: Convert a PDF document\n",
        "    PDF_PATH = \"https://arxiv.org/pdf/2212.10496v1\"\n",
        "\n",
        "    try:\n",
        "        # Convert document using Docling\n",
        "        converter = DocumentConverter()\n",
        "        doc = converter.convert(source=PDF_PATH).document\n",
        "\n",
        "        # Convert Docling document to dictionary representation\n",
        "        # (This depends on Docling's export capabilities)\n",
        "        doc_dict = doc.model_dump()  # or similar method depending on Docling version\n",
        "\n",
        "        # Process formulas\n",
        "        processed_doc = extract_and_map_formulas(doc_dict)\n",
        "\n",
        "        # Verify the changes\n",
        "        print(\"\\n=== Example Formula Items ===\")\n",
        "        if \"texts\" in processed_doc:\n",
        "            formula_items = [\n",
        "                item for item in processed_doc[\"texts\"]\n",
        "                if item.get(\"label\") == \"formula\"\n",
        "            ]\n",
        "            for idx, formula in enumerate(formula_items[:3]):  # Show first 3\n",
        "                print(f\"\\nFormula {idx + 1}:\")\n",
        "                print(f\"  Original (orig): {formula.get('orig', 'N/A')}\")\n",
        "                print(f\"  Text field (text): {formula.get('text', 'N/A')}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Document not found: {PDF_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing document: {e}\")\n",
        "        print(\"\\nTo use this script:\")\n",
        "        print(\"1. Install Docling: pip install docling\")\n",
        "        print(\"2. Replace PDF_PATH with your actual document path\")\n",
        "        print(\"3. Run the script\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuP9fqABYGDh",
        "outputId": "bee13fa7-0326-4932-8fd9-0e47f6ca681b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2025-12-15 10:10:11,971 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:10:11,974 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:10:12,045 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:10:12,046 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:10:12,556 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:10:12,557 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:10:12,563 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:10:12,564 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:10:12,764 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:10:12,766 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:10:12,907 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:10:12,909 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Formula Mapping Statistics ===\n",
            "formulas_processed: 8\n",
            "formulas_skipped: 0\n",
            "total_processed: 8\n",
            "\n",
            "=== Example Formula Items ===\n",
            "\n",
            "Formula 1:\n",
            "  Original (orig): sim ( q , d ) = ã€ˆ enc q ( q ) , enc d ( d ) ã€‰ = ã€ˆ v q , v d ã€‰ (1)\n",
            "  Text field (text): sim ( q , d ) = ã€ˆ enc q ( q ) , enc d ( d ) ã€‰ = ã€ˆ v q , v d ã€‰ (1)\n",
            "\n",
            "Formula 2:\n",
            "  Original (orig): f = enc d = enccon (2)\n",
            "  Text field (text): f = enc d = enccon (2)\n",
            "\n",
            "Formula 3:\n",
            "  Original (orig): v d = f ( d ) âˆ€ d âˆˆ D 1 âˆª D 2 âˆª ... âˆª D L (3)\n",
            "  Text field (text): v d = f ( d ) âˆ€ d âˆˆ D 1 âˆª D 2 âˆª ... âˆª D L (3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.document_converter import DocumentConverter\n",
        "from docling.datamodel.document import DoclingDocument\n",
        "\n",
        "class DoclingFormulaMapper:\n",
        "    def __init__(self):\n",
        "        self.processed = 0\n",
        "        self.skipped = 0\n",
        "\n",
        "    def process(self, doc: DoclingDocument) -> DoclingDocument:\n",
        "        for item in doc.texts:\n",
        "            if item.label == \"formula\":\n",
        "                if item.orig and not item.text:\n",
        "                    item.text = item.orig\n",
        "                    self.processed += 1\n",
        "                else:\n",
        "                    self.skipped += 1\n",
        "        return doc\n",
        "\n",
        "doc = DocumentConverter().convert(source=\"https://arxiv.org/pdf/2212.10496\").document\n",
        "\n",
        "mapper = DoclingFormulaMapper()\n",
        "processed_doc = mapper.process(doc)\n",
        "\n",
        "print(\"Formulas processed:\", mapper.processed)\n",
        "\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "chunker = HybridChunker(\n",
        "    max_tokens=3000,        # IMPORTANT (prevents 512 overflow)\n",
        ")\n",
        "\n",
        "chunk_iter = chunker.chunk(dl_doc=processed_doc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWZGn9XfcRL9",
        "outputId": "2d45426d-2066-4eb2-e9bf-c1b097631ed2"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2025-12-15 12:00:28,602 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 12:00:28,605 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 12:00:28,703 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 12:00:28,707 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 12:00:29,729 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 12:00:29,735 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 12:00:29,746 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 12:00:29,747 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 12:00:30,169 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 12:00:30,171 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 12:00:30,761 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 12:00:30,772 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formulas processed: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, chunk in enumerate(chunk_iter):\n",
        "    enriched_text = chunker.contextualize(chunk)\n",
        "    print(f\"=== Chunk {i} ===\")\n",
        "    print(enriched_text)\n"
      ],
      "metadata": {
        "id": "S7DZgKs3h6yo",
        "outputId": "a0c9046b-0b79-4c29-8b5d-57bfc552c082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Chunk 0 ===\n",
            "Precise Zero-Shot Dense Retrieval without Relevance Labels\n",
            "Luyu Gao âˆ— â€  Xueguang Ma âˆ— â€¡ Jimmy Lin â€¡ Jamie Callan â€ \n",
            "â€ \n",
            "Language Technologies Institute, Carnegie Mellon University â€¡ David R. Cheriton School of Computer Science, University of Waterloo {luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca\n",
            "=== Chunk 1 ===\n",
            "Abstract\n",
            "While dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings ( HyDE ). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT ) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder (e.g. Contriever ) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages (e.g. sw, ko, ja). 1\n",
            "=== Chunk 2 ===\n",
            "1 Introduction\n",
            "Dense retrieval (Lee et al., 2019; Karpukhin et al., 2020), the method of retrieving documents using semantic embedding similarities, has been shown successful across tasks like web search, question answering, and fact verification. A variety of methods such as negative mining (Xiong et al., 2021; Qu et al., 2021), distillation (Qu et al., 2021; Lin et al., 2021b; HofstÃ¤tter et al., 2021) and task-specific\n",
            "âˆ— Equal contribution.\n",
            "1 No models were trained or fine-tuned in making this preprint. Our open source code is available at https://github. com/texttron/hyde .\n",
            "pre-training (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) have been proposed to improve the effectiveness of supervised dense retrieval models.\n",
            "On the other hand, zero-shot dense retrieval still remains difficult. Many recent works consider the alternative transfer learning setup, where the dense retrievers are trained on a high-resource dataset and then evaluated on queries from new tasks. The MSMARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged querydocument pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in practice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO restricts commercial use and cannot be adopted in a variety of real-world search scenarios.\n",
            "In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and generalize across tasks. As supervision is not available, we start by examining self-supervised representation learning methods. Modern deep learning enables two distinct learning algorithms. At the token level, generative large language models (LLM) pretrained on large corpus have demonstrated strong natural language understanding (NLU) and generation (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse unseen instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). Ouyang et al. (2022) show that with a small amount of data, GPT-3 (Brown et al., 2020) models can be aligned\n",
            "Figure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries without changing the underlying GPT-3 and Contriever / mContriever models.\n",
            "to human intent to follow instructions.\n",
            "With these ingredients, we propose to pivot through Hypothetical Document Embeddings ( HyDE ), and decompose dense retrieval into two tasks, a generative task performed by an instruction-following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to \"write a document that answers the question\", i.e. a hypothetical document. We expect the generative process to capture \"relevance\" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder's dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner-product during contrastive training. Note that, interestingly, with HyDE factorization, the query-document similarity score is no longer explicitly modeled nor computed. Instead, the retrieval task is cast into two NLU and NLG tasks.\n",
            "HyDE appears unsupervised. No model is trained in HyDE : both the generative model and the contrastive encoder remain intact. Supervision signals were only involved in instruction learning of our backbone LLM.\n",
            "In our experiments, we show HyDE using InstructGPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) as backbone models significantly outperforms the previous state-of-the-art Contrieveronly zero-shot no-relevance system on 11 queries sets, covering tasks like Web Search, Question Answering, Fact Verification and languages like Swahili, Korean, Japanese.\n",
            "=== Chunk 3 ===\n",
            "2 Related Works\n",
            "Dense Retrieval (Lee et al., 2019; Karpukhin et al., 2020) has been extensively studied after the emergence of pre-trained Transformer language models (Devlin et al., 2019). Researchers studied the metric learning problems, such as training loss (Karpukhin et al., 2020) and negative sampling (Xiong et al., 2021; Qu et al., 2021), and also introduced distillation (Qu et al., 2021; Lin et al., 2021b; HofstÃ¤tter et al., 2021). Later works studied the second stage pre-training of language model specifically for retrieval (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022).\n",
            "The popularity of dense retrieval can be partially attributed to the rich and successful research in very efficient minimum inner product search (MIPS) at very large (billion) scales (Johnson et al., 2017).\n",
            "Instructions-Following Language Models Soon after the emergence of LLMs, several groups of researchers discover that LLMs trained on data consisting of instructions and their execution can zero-shot generalize to perform new tasks with new instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). This can be done by standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022).\n",
            "Concurrent to us, Asai et al. (2022) studied 'Task-aware Retrieval with Instructions'. They fine-tuned dense encoders that can also encode task-specific instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an instruction following generative LLM, as described above.\n",
            "Zero-Shot Dense Retrieval The tasks of zeroshot (dense) retrieval are arguably empirically defined by Thakur et al. (2021) for the neural retrieval community. Their BEIR benchmark consists of diverse retrieval tasks. The paper and many follow-up research generally consider the Transfer Learning setup where the dense retriever is first learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022).\n",
            "However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora.\n",
            "By the definition in Sachan et al. (2022), our setup can be roughly considered as 'unsupervised' . Strictly, as with Sachan et al. (2022), the only supervision resides in the LLM, in the processing of learning to follow instructions.\n",
            "Generative Retrieval Generative search is a new class of retrieval methods that use neural generative models as search indices (Metzler et al., 2021; Tay et al., 2022; Bevilacqua et al., 2022; Lee et al., 2022). These models use (constrained) decoding to generate document identifiers, such as id and sub-string, which map directly to real documents. They have to go through special training procedures over relevance data; effective search may also need to use novel forms of search indices (Bevilacqua et al., 2022; Lee et al., 2022). In comparison, our method uses the standard MIPS index and requires no training or training data. Our generative model produces an intermediate hypothetical document to be fed into a dense encoder, instead of a real document.\n",
            "=== Chunk 4 ===\n",
            "3 Methodology\n",
            "In this section, we first formally define the problem of (zero-shot) dense retrieval. Then we will introduce how HyDE is designed to solve it.\n",
            "=== Chunk 5 ===\n",
            "3.1 Preliminaries\n",
            "Dense retrieval models similarity between query and document with inner product similarity. Given a query q and document d , it uses two encoder function enc q and enc d to map them into d dimension vectors v q , v d , whose inner product is used as similarity measurement.\n",
            "$$sim ( q , d ) = ã€ˆ enc q ( q ) , enc d ( d ) ã€‰ = ã€ˆ v q , v d ã€‰ (1)$$\n",
            "For zero-shot retrieval, we consider L query sets Q 1 , Q 2 , ..., Q L and their corresponding search corpus, document sets D 1 , D 2 , ..., D L . Denote the j -th query from i -th set query set Q i as q ij . We need to fully define mapping functions enc q and enc d without access to any query set Q i , document set D i , or any relevance judgment r ij .\n",
            "The difficulty of zero-shot dense retrieval lies precisely in Equation 1: it requires learning of two embedding functions (for query and document respectively) into the same embedding space where inner product captures relevance . Without relevance judgments/scores to fit, learning becomes intractable.\n",
            "=== Chunk 6 ===\n",
            "3.2 HyDE\n",
            "HyDE circumvents the aforementioned learning problem by performing search in documentonly embedding space that captures documentdocument similarity. This can be easily learned using unsupervised contrastive learning (Izacard et al., 2021; Gao et al., 2021; Gao and Callan, 2022). We set document encoder enc d directly as a contrastive encoder enccon.\n",
            "$$f = enc d = enccon (2)$$\n",
            "This function is also denoted as f for simplicity. This unsupervised contrastive encoder will be shared by all incoming document corpus.\n",
            "$$v d = f ( d ) âˆ€ d âˆˆ D 1 âˆª D 2 âˆª ... âˆª D L (3)$$\n",
            "To build the query vector, we consider in addition an instruction following LM, InstructLM. It takes a query q and a textual instruction INST and follows them to perform the task specified by INST. For simplicity, denote,\n",
            "$$g ( q, INST ) = InstructLM ( q, INST ) (4)$$\n",
            "Now we can use g to map queries to \"hypothetical\" documents by sampling from g , setting INST\n",
            "to be 'write a paragraph that answers the question' . The generated document is not real, can and is likely to be ungrounded factually (Brown et al., 2020; Thoppilan et al., 2022). We only require it to capture relevance pattern. This is done by generating documents, i.e. providing examples. Critically, here we offload relevance modeling from representation learning model to an NLG model that generalizes significantly more easily, naturally, and effectively (Brown et al., 2020; Ouyang et al., 2022). Generating examples also replaces explicit modeling of relevance scores.\n",
            "We can now encode the generated document using the document encoder f . Write,\n",
            "$$E [ v q ij ] = E [ f ( g ( q ij , INST i ))] (5)$$\n",
            "Formally, g defines a probability distribution based on the chain rule. In this paper, we simply consider the expectation value, assuming the distribution of v q ij is uni-modal, i.e. the query is not ambiguous. The study of ambiguous queries and diversity is left to future work. We estimate Equation 5 by sampling N documents from g , [ Ë† d 1 , Ë† d 2 , ..., Ë† d N ] .\n",
            "$$Ë† v q ij = 1 N âˆ‘ Ë† d k âˆ¼ g ( q ij , INST i ) f ( d k ) (6)$$\n",
            "$$= 1 N N âˆ‘ k =1 f ( Ë† d k ) (7)$$\n",
            "Wealso consider the query as a possible hypothesis,\n",
            "$$Ë† v q ij = 1 N +1 [ N âˆ‘ k =1 f ( Ë† d k ) + f ( q ij )] (8)$$\n",
            "Inner product is computed between Ë† v q ij and the set of all document vectors { f ( d ) | d âˆˆ D i } . The most similar documents are retrieved. Here the encoder function f serves as a lossy compressor that outputs dense vectors, where the extra details are filtered and left out from the vector. It further grounds the hypothetical vector to the actual corpus and the real documents. The full HyDE system is illustrated in Figure 1.\n",
            "=== Chunk 7 ===\n",
            "4.1 Setup\n",
            "Implementation We implement HyDE using InstructGPT , a GPT-3 model from the instruct series ( text-davinci-003 ; Ouyang et al. (2022)) and Contriever models (Izacard et al., 2021). We sample from InstructGPT using the OpenAI playground default temperature of 0.7 for open-ended generations. We use the English-only Contriever model for English retrieval tasks and multilingual mContriever for non-English tasks. We conducted retrieval experiments with the Pyserini toolkit (Lin et al., 2021a).\n",
            "Datasets We consider web search query sets TREC DL19 (Craswell et al., 2020a) and DL20 (Craswell et al., 2020b); they are based on the MS-MARCO dataset (Bajaj et al., 2016). We also use a diverse collection of 6 low-resource datasets from the BEIR dataset (Thakur et al., 2021). For non-English retrieval, we consider Swahili, Korean, Japanese, and Bengali from the Mr.Tydi dataset (Zhang et al., 2021).\n",
            "We use different instructions for each dataset. They share a similar structure but have different quantifiers to control the exact form of the generated hypothetical documents. These instructions can be found in subsection A.1.\n",
            "Compared Systems Contriever models, Contriever and mContriever , serve as our major baseline. They are trained using unsupervised contrastive learning. HyDE retrievers share the exact same embedding spaces with them. The only difference is how the query vector is built. These comparisons allow us to easily examine the effect of HyDE . The classical heuristic-based lexical retriever BM25 is also included.\n",
            "Several systems that involve fine-tuning on massive relevance data are also included as references. We consider models fine-tuned on MSMARCO and transferred, DPR and ANCE, from the BEIR paper. For multilingual, we include the mDPR model from Mr.Tydi paper and MSMARCO fine-tuned mBERT and XLM-R from the Contriever paper. We also include the state-ofthe-art transfer learning models: Contriever and mContriever fine-tuned on MS-MARCO, denoted Contriever FT and mContriever FT . These models have run through the state-of-the-art retrieval model training pipeline that involves second-stage retrieval-specific pre-training (Lee et al., 2019) and a few rounds of fine-tuning (Qu et al., 2021); they should be considered empirical upper bounds.\n",
            "=== Chunk 8 ===\n",
            "4.2 Web Search\n",
            "In Table 1, we show retrieval results on TREC DL19 and TREC DL20. We see HyDE bring sizable improvements to Contriever across the board for\n",
            "Table 1: Results for web search on DL19/20. Best performing w/o relevance and overall system(s) are marked bold . DPR, ANCE and Contriever FT are in-domain supervised models that are finetuned on MS MARCO training data.\n",
            "\n",
            "w/o relevance judgement, DL19.map = w/o relevance judgement. w/o relevance judgement, DL19.ndcg@10 = w/o relevance judgement. w/o relevance judgement, DL19.recall@1k = . w/o relevance judgement, DL20.map = . w/o relevance judgement, DL20.ndcg@10 = . w/o relevance judgement, DL20.recall@1k = . BM25, DL19.map = 30.1. BM25, DL19.ndcg@10 = 50.6. BM25, DL19.recall@1k = 75.0. BM25, DL20.map = 28.6. BM25, DL20.ndcg@10 = 48.0. BM25, DL20.recall@1k = 78.6. Contriever, DL19.map = 24.0. Contriever, DL19.ndcg@10 = 44.5. Contriever, DL19.recall@1k = 74.6. Contriever, DL20.map = 24.0. Contriever, DL20.ndcg@10 = 42.1. Contriever, DL20.recall@1k = 75.4. HyDE, DL19.map = 41.8. HyDE, DL19.ndcg@10 = 61.3. HyDE, DL19.recall@1k = 88.0. HyDE, DL20.map = 38.2. HyDE, DL20.ndcg@10 = 57.9. HyDE, DL20.recall@1k = 84.4. w/ relevance judgement, DL19.map = w/ relevance judgement. w/ relevance judgement, DL19.ndcg@10 = w/ relevance judgement. w/ relevance judgement, DL19.recall@1k = . w/ relevance judgement, DL20.map = . w/ relevance judgement, DL20.ndcg@10 = . w/ relevance judgement, DL20.recall@1k = . DPR, DL19.map = 36.5. DPR, DL19.ndcg@10 = 62.2. DPR, DL19.recall@1k = 76.9. DPR, DL20.map = 41.8. DPR, DL20.ndcg@10 = 65.3. DPR, DL20.recall@1k = 81.4. ANCE, DL19.map = 37.1. ANCE, DL19.ndcg@10 = 64.5. ANCE, DL19.recall@1k = 75.5. ANCE, DL20.map = 40.8. ANCE, DL20.ndcg@10 = 64.6. ANCE, DL20.recall@1k = 77.6. Contriever FT, DL19.map = 41.7. Contriever FT, DL19.ndcg@10 = 62.1. Contriever FT, DL19.recall@1k = 83.6. Contriever FT, DL20.map = 43.6. Contriever FT, DL20.ndcg@10 = 63.2. Contriever FT, DL20.recall@1k = 85.8\n",
            "Table 2: Low resource tasks from BEIR. Best performing w/o relevance and overall system(s) are marked bold .\n",
            "\n",
            "nDCG@10, Scifact = nDCG@10. nDCG@10, Arguana = nDCG@10. nDCG@10, Trec-Covid = nDCG@10. nDCG@10, FiQA = nDCG@10. nDCG@10, DBPedia = nDCG@10. nDCG@10, TREC-NEWS = nDCG@10. w/o relevance judgement, Scifact = w/o relevance judgement. w/o relevance judgement, Arguana = w/o relevance judgement. w/o relevance judgement, Trec-Covid = w/o relevance judgement. w/o relevance judgement, FiQA = w/o relevance judgement. w/o relevance judgement, DBPedia = w/o relevance judgement. w/o relevance judgement, TREC-NEWS = w/o relevance judgement. BM25, Scifact = 67.9. BM25, Arguana = 39.7. BM25, Trec-Covid = 59.5. BM25, FiQA = 23.6. BM25, DBPedia = 31.8. BM25, TREC-NEWS = 39.5. Contriever, Scifact = 64.9. Contriever, Arguana = 37.9. Contriever, Trec-Covid = 27.3. Contriever, FiQA = 24.5. Contriever, DBPedia = 29.2. Contriever, TREC-NEWS = 34.8. HyDE, Scifact = 69.1. HyDE, Arguana = 46.6. HyDE, Trec-Covid = 59.3. HyDE, FiQA = 27.3. HyDE, DBPedia = 36.8. HyDE, TREC-NEWS = 44.0. w/ relevance judgement, Scifact = w/ relevance judgement. w/ relevance judgement, Arguana = w/ relevance judgement. w/ relevance judgement, Trec-Covid = w/ relevance judgement. w/ relevance judgement, FiQA = w/ relevance judgement. w/ relevance judgement, DBPedia = w/ relevance judgement. w/ relevance judgement, TREC-NEWS = w/ relevance judgement. DPR, Scifact = 31.8. DPR, Arguana = 17.5. DPR, Trec-Covid = 33.2. DPR, FiQA = 29.5. DPR, DBPedia = 26.3. DPR, TREC-NEWS = 16.1. ANCE, Scifact = 50.7. ANCE, Arguana = 41.5. ANCE, Trec-Covid = 65.4. ANCE, FiQA = 30.0. ANCE, DBPedia = 28.1. ANCE, TREC-NEWS = 38.2. Contriever FT, Scifact = 67.7. Contriever FT, Arguana = 44.6. Contriever FT, Trec-Covid = 59.6. Contriever FT, FiQA = 32.9. Contriever FT, DBPedia = 41.3. Contriever FT, TREC-NEWS = 42.8. Recall@100, Scifact = Recall@100. Recall@100, Arguana = Recall@100. Recall@100, Trec-Covid = Recall@100. Recall@100, FiQA = Recall@100. Recall@100, DBPedia = Recall@100. Recall@100, TREC-NEWS = Recall@100. w/o relevance judgement, Scifact = w/o relevance judgement. w/o relevance judgement, Arguana = w/o relevance judgement. w/o relevance judgement, Trec-Covid = w/o relevance judgement. w/o relevance judgement, FiQA = w/o relevance judgement. w/o relevance judgement, DBPedia = w/o relevance judgement. w/o relevance judgement, TREC-NEWS = w/o relevance judgement. BM25, Scifact = 92.5. BM25, Arguana = 93.2. BM25, Trec-Covid = 49.8. BM25, FiQA = 54.0. BM25, DBPedia = 46.8. BM25, TREC-NEWS = 44.7. Contriever, Scifact = 92.6. Contriever, Arguana = 90.1. Contriever, Trec-Covid = 17.2. Contriever, FiQA = 56.2. Contriever, DBPedia = 45.3. Contriever, TREC-NEWS = 42.3. HyDE, Scifact = 96.4. HyDE, Arguana = 97.9. HyDE, Trec-Covid = 41.4. HyDE, FiQA = 62.1. HyDE, DBPedia = 47.2. HyDE, TREC-NEWS = 50.9. w/ relevance judgement, Scifact = w/ relevance judgement. w/ relevance judgement, Arguana = w/ relevance judgement. w/ relevance judgement, Trec-Covid = w/ relevance judgement. w/ relevance judgement, FiQA = w/ relevance judgement. w/ relevance judgement, DBPedia = w/ relevance judgement. w/ relevance judgement, TREC-NEWS = w/ relevance judgement. DPR, Scifact = 72.7. DPR, Arguana = 75.1. DPR, Trec-Covid = 21.2. DPR, FiQA = 34.2. DPR, DBPedia = 34.9. DPR, TREC-NEWS = 21.5. ANCE, Scifact = 81.6. ANCE, Arguana = 93.7. ANCE, Trec-Covid = 45.7. ANCE, FiQA = 58.1. ANCE, DBPedia = 31.9. ANCE, TREC-NEWS = 39.8. Contriever FT, Scifact = 94.7. Contriever FT, Arguana = 97.7. Contriever FT, Trec-Covid = 40.7. Contriever FT, FiQA = 65.6. Contriever FT, DBPedia = 54.1. Contriever FT, TREC-NEWS = 49.2\n",
            "both precision-oriented and recall metrics. While unsupervised Contriever can underperform the classical BM25 approach, HyDE outperforms BM25 by large margins.\n",
            "HyDE remains competitive even when compared to fine-tuned models. Note that TREC DL19/20 are search tasks defined on MS-MARCO and there, all the fine-tuned models are richly supervised . On TREC DL19, HyDE shows comparable map and ndcg@10 to Contriever FT and best recall@1k. On DL20, HyDE gets around 10% lower map and ndcg@10 than Contriever FT and similar recall@1k. The ANCE model shows better ndcg@10 numbers than HyDE but lower recall, suggesting it may be biased to a subset of queries and/or relevant documents.\n",
            "=== Chunk 9 ===\n",
            "4.3 Low Resource Retrieval\n",
            "In Table 2, we show retrieval results on lowresource tasks from BEIR. Similar to web search, HyDE again brings sizable improvements to Contriever across the board in terms of both ndcg and recall. HyDE is only outperformed by BM25 on one dataset, TREC-Covid but with a tiny 0.2 margin; in comparison, the underlying Contriever underperforms by more than 50%.\n",
            "We also observe HyDE demonstrates strong performance compared to fine-tuned models. HyDE generally shows better performance than ANCE and DPR, even though the two are fine-tuned on MS-MARCO and ANCE also involves some sophisticated hard negative techniques. Contriever FT shows performance advantages on FiQA and DBPedia. These involve retrieval of financial posts or entities respectively. We believe the performance difference can be attributed to the\n",
            "Table 3: MRR@100 on Mr.Tydi. Best performing w/o relevance and overall system(s) are marked bold .\n",
            "\n",
            "w/o relevance judgement, Swahili = w/o relevance judgement. w/o relevance judgement, Korean = . w/o relevance judgement, Japanese = . w/o relevance judgement, Bengali = . BM25, Swahili = 38.9. BM25, Korean = 28.5. BM25, Japanese = 21.2. BM25, Bengali = 41.8. mContriever, Swahili = 38.3. mContriever, Korean = 22.3. mContriever, Japanese = 19.5. mContriever, Bengali = 35.3. HyDE, Swahili = 41.7. HyDE, Korean = 30.6. HyDE, Japanese = 30.7. HyDE, Bengali = 41.3. w/ relevance judgement, Swahili = w/ relevance judgement. w/ relevance judgement, Korean = . w/ relevance judgement, Japanese = . w/ relevance judgement, Bengali = . mDPR, Swahili = 7.3. mDPR, Korean = 21.9. mDPR, Japanese = 18.1. mDPR, Bengali = 25.8. mBERT, Swahili = 37.4. mBERT, Korean = 28.1. mBERT, Japanese = 27.1. mBERT, Bengali = 35.1. XLM-R, Swahili = 35.1. XLM-R, Korean = 32.2. XLM-R, Japanese = 24.8. XLM-R, Bengali = 41.7. mContriever FT, Swahili = 51.2. mContriever FT, Korean = 34.2. mContriever FT, Japanese = 32.4. mContriever FT, Bengali = 42.3\n",
            "under-specification of the instruction; more elaborative instructions may help.\n",
            "=== Chunk 10 ===\n",
            "4.4 Multilingual Retrieval\n",
            "Multilingual setup poses several additional challenges to HyDE . The small-sized contrastive encoder gets saturated as the number of languages scales (Conneau et al., 2020; Izacard et al., 2021). Meanwhile, our generative LLM faces an opposite issue: with languages of not as high resource as English or French, the high capacity LLM can get under-trained (Hoffmann et al., 2022).\n",
            "Nevertheless, in Table 3, we still find HyDE able to improve the mContriever model. It can outperform non-Contriever models fine-tuned on and transferred from MS-MARCO. On the other hand, we do observe some margins between HyDE and fine-tuned mContriever FT . Since HyDE and mContriever FT use similar contrastive encoders, we hypothesize this is because the non-English languages we considered are under-trained in both pre-training and instruction learning stages.\n",
            "=== Chunk 11 ===\n",
            "5 Analysis\n",
            "The generative LLM and contrastive encoder make up the backbone of HyDE . In this section, we study the effect of changing their realizations. In particular, we consider smaller language models (LM) and fine-tuned encoders. We conduct our studies on TREC DL19/20.\n",
            "=== Chunk 12 ===\n",
            "5.1 Effect of Different Generative Models\n",
            "In Table 4, we show HyDE using other instruction-following language models. In particular, we consider a 52-billion Cohere model ( command-xlarge-20221108 ) and a 11-billion FLAN model ( FLAN-T5-xxl ; Wei et al. (2022)). 2 Generally, we observe that all\n",
            "2 Model sizes are from https://crfm.stanford.edu/ helm/v1.0/?models .\n",
            "Table 4: NDCG@10 on TREC DL19/20. Effect of changing different instruction LMs and using finetuned encoder. Best w/o relevance and overall models are marked bold .\n",
            "\n",
            "Contriever, DL19 = 44.5. Contriever, DL20 = 42.1. Contriever FT, DL19 = 62.1. Contriever FT, DL20 = 63.2. HyDE, DL19 = . HyDE, DL20 = . w/ Contriever, DL19 = . w/ Contriever, DL20 = . w/ Flan-T5 (11b), DL19 = 48.9. w/ Flan-T5 (11b), DL20 = 52.9. w/ Cohere (52b), DL19 = 53.8. w/ Cohere (52b), DL20 = 53.8. w/ GPT (175b), DL19 = 61.3. w/ GPT (175b), DL20 = 57.9. w/ Contriever FT, DL19 = . w/ Contriever FT, DL20 = . w/ Flan-T5 (11b), DL19 = 60.2. w/ Flan-T5 (11b), DL20 = 62.1. w/ Cohere (52b), DL19 = 61.4. w/ Cohere (52b), DL20 = 63.1. w/ GPT (175b), DL19 = 67.4. w/ GPT (175b), DL20 = 63.5\n",
            "models bring improvement to the unsupervised Contriever , with larger models bringing larger improvements. At the time when this paper is written, the Cohere model is still experimental without much detail disclosed. We can only tentatively hypothesize that training techniques may have also played some role in the performance difference.\n",
            "=== Chunk 13 ===\n",
            "5.2 HyDE with Fine-tuned Encoder\n",
            "To begin with, HyDE with fine-tuned encoder is not the intended usage: HyDE is more powerful and irreplaceable when few relevance labels are present. Here we are interested to find out if and how HyDE embedding can affect fine-tuned encoders. In Table 4, we see that less powerful instruction LMs can negatively impact the overall performance of the fine-tuned retriever. (To remind our readers, Contriever FT is in-domain supervisedly fine-tuned for TREC DL19/20). The performance degradations remain small. On the other hand, we also observe the InstructGPT model able to further bring up the performance, especially on DL19. This suggests that there may still exist certain factors not captured by the fine-tuned encoder but only by the generative model.\n",
            "=== Chunk 14 ===\n",
            "6 Conclusion\n",
            "At the end of the paper, we encourage the readers to take a moment and reflect on the HyDE model. Compare it to some of the other recently seen retrievers or re-ranker. These other models probably differ in their architecture, training method, and/or task, but probably all of them involve modeling relevance scores between a pair of query and docu- ment. Dense retrievers consider vector similarities while self-attentive re-rankers regression scores. In comparison, the concept of relevance in HyDE is captured by an NLG model and the language generation process. We demonstrate in many cases, HyDE can be as effective as dense retrievers that learn to model numerical relevance scores. So, is numerical relevance just a statistical artifact of language understanding? Will a weak retriever theoretically suffice as the NLU & NLG models rapidly become stronger? Rushing to conclusions is not smart; more works need to be done to get answers. With this paper, we just want to raise these questions.\n",
            "Concretely in this paper, we introduce a new paradigm of interactions between LLM and dense encoder/retriever. We demonstrate (part of) relevance modeling and instruction understanding can be delegated to the more powerful and flexible LLM. As a consequence, the need for relevance labels is removed. We are excited to see how this can be generalized further to more sophisticated tasks like multi-hop retrieval/QA and conversational search.\n",
            "Weargue HyDE is also of practical use though not necessarily over the entire lifespan of a search system. At the very beginning of the life of the search system, serving queries using HyDE offers performance comparable to a fine-tuned model, which no other relevance-free model can offer. As the search log grows, a supervised dense retriever can be gradually rolled out. As the dense retriever grows stronger, more queries will be routed to it, with only less common and emerging ones going to HyDE backend.\n",
            "=== Chunk 15 ===\n",
            "References\n",
            "Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. Task-aware retrieval with instructions.\n",
            "Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2016. Ms marco: A human generated machine reading comprehension dataset.\n",
            "Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive search engines: Generating substrings as document identifiers. CoRR , abs/2204.10628.\n",
            "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\n",
            "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.\n",
            "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.\n",
            "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettle-\n",
            "- moyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 84408451, Online. Association for Computational Linguistics.\n",
            "- Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2020a. Overview of the trec 2019 deep learning track.\n",
            "- Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. Voorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820.\n",
            "- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n",
            "- Luyu Gao and Jamie Callan. 2021. Condenser: a pretraining architecture for dense retrieval. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 981-993, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n",
            "- Luyu Gao and Jamie Callan. 2022. Unsupervised corpus aware language model pre-training for dense passage retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2843-2853, Dublin, Ireland. Association for Computational Linguistics.\n",
            "- Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n",
            "- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models.\n",
            "- Sebastian HofstÃ¤tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval ,\n",
            "- SIGIR '21, page 113-122, New York, NY, USA. Association for Computing Machinery.\n",
            "- Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. CoRR , abs/2112.09118.\n",
            "- Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2017. Billion-scale similarity search with gpus. CoRR , abs/1702.08734.\n",
            "- Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 67696781, Online. Association for Computational Linguistics.\n",
            "- Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. 2022. Generative multi-hop retrieval.\n",
            "- Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6086-6096, Florence, Italy. Association for Computational Linguistics.\n",
            "- Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021a. Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021) , pages 2356-2362.\n",
            "- Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021b. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021) , pages 163173, Online. Association for Computational Linguistics.\n",
            "- Zheng Liu and Yingxia Shao. 2022. Retromae: Pretraining retrieval-oriented transformers via masked auto-encoder. ArXiv , abs/2205.12035.\n",
            "- Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold Overwijk. 2021. Less is more: Pretrain a strong Siamese encoder for dense text retrieval using a weak decoder. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 2780-2791, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n",
            "- Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 2021. Rethinking search: making domain experts out of dilettantes. SIGIR Forum , 55(1):13:1-13:27.\n",
            "Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2791-2809, Seattle, United States. Association for Computational Linguistics.\n",
            "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.\n",
            "Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 5835-5847, Online. Association for Computational Linguistics.\n",
            "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis & insights from training gopher.\n",
            "=== Chunk 16 ===\n",
            "References\n",
            "Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage retrieval with zero-shot question generation.\n",
            "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault FÃ©vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net.\n",
            "Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Prakash Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022. Transformer memory as a differentiable search index. CoRR , abs/2202.06991.\n",
            "Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. CoRR , abs/2104.08663.\n",
            "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications. CoRR , abs/2201.08239.\n",
            "Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022. GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2345-2360, Seattle, United States. Association for Computational Linguistics.\n",
            "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net.\n",
            "- Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net.\n",
            "- Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and Arnold Overwijk. 2022. Coco-dr: Combating distribution shifts in zero-shot dense retrieval with contrastive and distributionally robust learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing .\n",
            "- Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021. Mr. TyDi: A multi-lingual benchmark for dense retrieval. arXiv:2108.08787 .\n",
            "=== Chunk 17 ===\n",
            "A.1.1 Web Search\n",
            "Please write a passage to answer the question\n",
            "Question: [QUESTION]\n",
            "Passage:\n",
            "=== Chunk 18 ===\n",
            "A.1.2 SciFact\n",
            "Please write a scientific paper passage to support/refute the claim\n",
            "Claim: [Claim]\n",
            "Passage:\n",
            "=== Chunk 19 ===\n",
            "A.1.3 Arguana\n",
            "Please write a counter argument for the passage\n",
            "Passage: [PASSAGE]\n",
            "Counter Argument:\n",
            "=== Chunk 20 ===\n",
            "A.1.4 TREC-COVID\n",
            "Please write a scientific paper passage to answer the question\n",
            "Question: [QUESTION]\n",
            "Passage:\n",
            "=== Chunk 21 ===\n",
            "A.1.5 FiQA\n",
            "Please write a financial article passage to answer the question\n",
            "Question: [QUESTION]\n",
            "Passage:\n",
            "=== Chunk 22 ===\n",
            "A.1.6 DBPedia-Entity\n",
            "Please write a passage to answer the question.\n",
            "Question: [QUESTION]\n",
            "Passage:\n",
            "=== Chunk 23 ===\n",
            "A.1.7 TREC-NEWS\n",
            "Please write a news passage about the topic.\n",
            "Topic: [TOPIC]\n",
            "Passage:\n",
            "=== Chunk 24 ===\n",
            "A.1.8 Mr.TyDi\n",
            "Please write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail.\n",
            "Question: [QUESTION]\n",
            "Passage:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, chunk in enumerate(chunk_iter):\n",
        "    print(f\"=== {i} ===\")\n",
        "    print(f\"chunk.text:\\n{f'{chunk.text[:300]}â€¦'!r}\")\n",
        "\n",
        "    enriched_text = chunker.contextualize(chunk=chunk)\n",
        "    print(f\"chunker.contextualize(chunk):\\n{f'{enriched_text[:300]}â€¦'!r}\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vfOj7SkscuSk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "hh3QKOsNZp-F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_docling.loader import ExportType\n",
        "\n",
        "\n",
        "def _get_env_from_colab_or_os(key):\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "\n",
        "        try:\n",
        "            return userdata.get(key)\n",
        "        except userdata.SecretNotFoundError:\n",
        "            pass\n",
        "    except ImportError:\n",
        "        pass\n",
        "    return os.getenv(key)\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# https://github.com/huggingface/transformers/issues/5486:\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "HF_TOKEN = _get_env_from_colab_or_os(\"HF_TOKEN\")\n",
        "FILE_PATH = [\"https://arxiv.org/pdf/2212.10496\"]  # Docling Technical Report\n",
        "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "GEN_MODEL_ID = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "EXPORT_TYPE = ExportType.DOC_CHUNKS\n",
        "QUESTION = \"Which are the main AI models in Docling?\"\n",
        "PROMPT = PromptTemplate.from_template(\n",
        "    \"Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {input}\\nAnswer:\\n\",\n",
        ")\n",
        "TOP_K = 7\n",
        "MILVUS_URI = str(Path(mkdtemp()) / \"docling.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2jWyb31Zp-I"
      },
      "source": [
        "## Document loading\n",
        "\n",
        "Now we can instantiate our loader and load documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbizbrTsZp-J",
        "outputId": "d5eb9cac-aba3-4925-a440-575829769a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2025-12-15 10:34:39,499 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:34:39,501 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:34:39,581 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:34:39,583 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:34:39,985 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:34:39,987 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:34:39,994 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:34:39,996 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:34:40,133 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:34:40,134 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:34:40,273 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-15 10:34:40,274 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "from langchain_docling import DoclingLoader\n",
        "\n",
        "from docling.chunking import HybridChunker\n",
        "\n",
        "loader = DoclingLoader(\n",
        "    file_path=FILE_PATH,\n",
        "    export_type=EXPORT_TYPE,\n",
        "    chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
        ")\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9tIGjpIZp-Q"
      },
      "source": [
        "> Note: a message saying `\"Token indices sequence length is longer than the specified\n",
        "maximum sequence length...\"` can be ignored in this case â€” details\n",
        "[here](https://github.com/docling-project/docling-core/issues/119#issuecomment-2577418826)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bZsJPkkzyNnE",
        "outputId": "e0f81aef-c82f-4d81-fea1-2aefa35eb808"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/3', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 113.643, 't': 481.532, 'r': 498.359, 'b': 439.849, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 295]}]}, {'self_ref': '#/texts/4', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 249.283, 't': 427.545, 'r': 362.717, 'b': 408.084, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 50]}]}], 'headings': ['Version 1.0'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Version 1.0\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\nAI4K Group, IBM Research RÂ¨ uschlikon, Switzerland'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/6', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 143.865, 't': 364.013, 'r': 468.138, 'b': 300.737, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 431]}]}], 'headings': ['Abstract'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Abstract\\nThis technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/8', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 108.0, 't': 239.37, 'r': 504.003, 'b': 143.54600000000005, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 792]}]}, {'self_ref': '#/texts/9', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 108.0, 't': 135.88800000000003, 'r': 504.003, 'b': 83.52099999999996, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 488]}]}, {'self_ref': '#/texts/12', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 108.0, 't': 716.523, 'r': 253.972, 'b': 707.971, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 36]}]}], 'headings': ['1 Introduction'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='1 Introduction\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\nHere is what Docling delivers today:'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/13', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 2, 'bbox': {'l': 135.397, 't': 695.23, 'r': 468.397, 'b': 686.678, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 78]}]}, {'self_ref': '#/texts/14', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 2, 'bbox': {'l': 135.397, 't': 680.366, 'r': 504.003, 'b': 660.905, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 96]}]}, {'self_ref': '#/texts/15', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 2, 'bbox': {'l': 135.397, 't': 654.593, 'r': 480.85, 'b': 646.041, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 86]}]}, {'self_ref': '#/texts/16', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 2, 'bbox': {'l': 135.397, 't': 639.729, 'r': 333.463, 'b': 631.177, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 47]}]}, {'self_ref': '#/texts/17', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 2, 'bbox': {'l': 135.397, 't': 624.866, 'r': 504.003, 'b': 605.405, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 161]}]}, {'self_ref': '#/texts/18', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 2, 'bbox': {'l': 135.397, 't': 599.093, 'r': 355.411, 'b': 590.541, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 54]}]}], 'headings': ['1 Introduction'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='1 Introduction\\n- Converts PDF documents to JSON or Markdown format, stable and lightning fast\\n- Understands detailed page layout, reading order, locates figures and recovers table structures\\n- Extracts metadata from the document, such as title, authors, references and language\\n- Optionally applies OCR, e.g. for scanned PDFs\\n- Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\n- Can leverage different accelerators (GPU, MPS, etc).'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/20', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 108.0, 't': 547.82, 'r': 504.003, 'b': 506.362, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 321]}]}, {'self_ref': '#/texts/21', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 108.0, 't': 498.525, 'r': 504.003, 'b': 457.246, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 371]}]}, {'self_ref': '#/texts/22', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 108.753, 't': 448.911, 'r': 423.447, 'b': 441.442, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 56]}]}], 'headings': ['2 Getting Started'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='2 Getting Started\\nTo use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\\nDocling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\\nfrom docling.document_converter import DocumentConverter'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/23', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'code', 'prov': [{'page_no': 2, 'bbox': {'l': 108.785, 't': 428.985, 'r': 491.336, 'b': 381.666, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 265]}]}, {'self_ref': '#/texts/24', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 108.0, 't': 367.837, 'r': 504.003, 'b': 315.649, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 403]}]}], 'headings': ['2 Getting Started'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='2 Getting Started\\n```\\nsource = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Large Human -Annotated Dataset for Document -Layout Analysis [...]\"\\n```\\nOptionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/26', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 108.0, 't': 272.749, 'r': 504.003, 'b': 176.92399999999998, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 796]}]}], 'headings': ['3 Processing pipeline'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='3 Processing pipeline\\nDocling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/28', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 108.0, 't': 141.07100000000003, 'r': 504.003, 'b': 88.88200000000006, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 487]}]}, {'self_ref': '#/texts/29', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'footnote', 'prov': [{'page_no': 2, 'bbox': {'l': 120.653, 't': 79.70000000000005, 'r': 276.461, 'b': 70.13999999999999, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 42]}]}, {'self_ref': '#/texts/31', 'parent': {'$ref': '#/pictures/1'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 3, 'bbox': {'l': 108.0, 't': 570.003, 'r': 504.003, 'b': 550.542, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 134]}]}, {'self_ref': '#/texts/47', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 108.0, 't': 524.405, 'r': 504.003, 'b': 504.943, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 173]}]}], 'headings': ['3.1 PDF backends'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content=\"3.1 PDF backends\\nTwo basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\\n1 see huggingface.co/ds4sd/docling-models/\\nFigure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\\nlicensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\"),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/48', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 108.0, 't': 497.107, 'r': 504.003, 'b': 444.919, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 446]}]}], 'headings': ['3.1 PDF backends'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='3.1 PDF backends\\nWe therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/50', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 108.0, 't': 404.873, 'r': 504.003, 'b': 330.866, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 608]}]}], 'headings': ['3.2 AI models'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='3.2 AI models\\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/52', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 108.0, 't': 293.51, 'r': 504.003, 'b': 252.231, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 388]}]}, {'self_ref': '#/texts/53', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 108.0, 't': 244.394, 'r': 504.003, 'b': 192.20600000000002, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 443]}]}], 'headings': ['Layout Analysis Model'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Layout Analysis Model\\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/55', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 108.0, 't': 154.85000000000002, 'r': 504.003, 'b': 69.93399999999997, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 706]}]}, {'self_ref': '#/texts/57', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 108.0, 't': 716.523, 'r': 504.003, 'b': 664.335, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 459]}]}], 'headings': ['Table Structure Recognition'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Table Structure Recognition\\nThe TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/59', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 108.0, 't': 631.616, 'r': 504.003, 'b': 568.518, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 515]}]}, {'self_ref': '#/texts/60', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 108.0, 't': 560.682, 'r': 504.003, 'b': 541.221, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 139]}]}], 'headings': ['OCR'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='OCR\\nDocling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\\nWe are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/62', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 108.0, 't': 505.812, 'r': 504.003, 'b': 431.805, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 622]}]}], 'headings': ['3.3 Assembly'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='3.3 Assembly\\nIn the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/64', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 108.0, 't': 396.396, 'r': 504.003, 'b': 311.48, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 753]}]}, {'self_ref': '#/texts/65', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 108.0, 't': 303.644, 'r': 504.003, 'b': 262.365, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 330]}]}], 'headings': ['3.4 Extensibility'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='3.4 Extensibility\\nDocling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\\nImplementations of model classes must satisfy the python Callable interface. The __call__ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/67', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 108.0, 't': 220.35400000000004, 'r': 504.003, 'b': 135.438, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 750]}]}, {'self_ref': '#/texts/68', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 108.0, 't': 127.60199999999998, 'r': 504.003, 'b': 97.23199999999997, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 290]}]}], 'headings': ['4 Performance'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='4 Performance\\nIn this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\\nIf you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/69', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 108.0, 't': 89.39499999999998, 'r': 504.003, 'b': 69.93399999999997, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 192]}]}, {'self_ref': '#/texts/71', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 108.0, 't': 716.523, 'r': 504.003, 'b': 697.062, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 121]}]}, {'self_ref': '#/texts/72', 'parent': {'$ref': '#/tables/0'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 5, 'bbox': {'l': 108.0, 't': 685.348, 'r': 504.003, 'b': 644.069, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 383]}]}, {'self_ref': '#/tables/0', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/72'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 133.27708435058594, 't': 634.9401397705078, 'r': 478.2610168457031, 'b': 542.4000701904297, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4 Performance'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='4 Performance\\nEstablishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\\ntorch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\\n\\nTable 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/0', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/72'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 133.27708435058594, 't': 634.9401397705078, 'r': 478.2610168457031, 'b': 542.4000701904297, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4 Performance'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='4 Performance\\nApple M3 Max (16 cores), Thread budget. = 4 16. Apple M3 Max (16 cores), native backend.TTS = 177 s 167 s. Apple M3 Max (16 cores), native backend.Pages/s = 1.27 1.34. Apple M3 Max (16 cores), native backend.Mem = 6.20 GB. Apple M3 Max (16 cores), pypdfium backend.TTS = 103 s 92 s. Apple M3 Max (16 cores), pypdfium backend.Pages/s = 2.18 2.45. Apple M3 Max (16 cores), pypdfium backend.Mem = 2.56 GB. Intel(R) Xeon E5-2690, Thread budget. = 4 16. Intel(R) Xeon E5-2690, native backend.TTS = 375 s 244 s. Intel(R) Xeon E5-2690, native backend.Pages/s = 0.60 0.92. Intel(R) Xeon E5-2690, native backend.Mem = 6.16 GB. Intel(R) Xeon'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/0', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/72'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 133.27708435058594, 't': 634.9401397705078, 'r': 478.2610168457031, 'b': 542.4000701904297, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4 Performance'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='4 Performance\\nE5-2690, pypdfium backend.TTS = 239 s 143 s. Intel(R) Xeon E5-2690, pypdfium backend.Pages/s = 0.94 1.57. Intel(R) Xeon E5-2690, pypdfium backend.Mem = 2.42 GB'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/74', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 108.0, 't': 504.04, 'r': 504.003, 'b': 364.579, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1189]}]}], 'headings': ['5 Applications'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content=\"5 Applications\\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\"),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/76', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 108.0, 't': 322.2, 'r': 504.003, 'b': 259.10300000000007, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 543]}]}, {'self_ref': '#/texts/77', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 108.0, 't': 251.654, 'r': 504.003, 'b': 199.07799999999997, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 402]}]}], 'headings': ['6 Future work and contributions'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='6 Future work and contributions\\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/79', 'parent': {'$ref': '#/groups/1'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 5, 'bbox': {'l': 112.981, 't': 162.17899999999997, 'r': 504.001, 'b': 142.572, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 127]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='References\\n- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/80', 'parent': {'$ref': '#/groups/1'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 5, 'bbox': {'l': 112.981, 't': 133.03200000000004, 'r': 504.003, 'b': 69.93399999999997, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 543]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='References\\n- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2:'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/80', 'parent': {'$ref': '#/groups/1'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 5, 'bbox': {'l': 112.981, 't': 133.03200000000004, 'r': 504.003, 'b': 69.93399999999997, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 543]}]}, {'self_ref': '#/texts/82', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 6, 'bbox': {'l': 129.579, 't': 716.523, 'r': 504.003, 'b': 675.098, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 331]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content=\"References\\nFaster\\nmachine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\"),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/83', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 112.981, 't': 664.917, 'r': 504.003, 'b': 634.547, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 255]}]}, {'self_ref': '#/texts/84', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 112.981, 't': 624.22, 'r': 504.0, 'b': 604.612, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 113]}]}, {'self_ref': '#/texts/85', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 112.981, 't': 594.431, 'r': 478.887, 'b': 585.733, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 84]}]}, {'self_ref': '#/texts/86', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 112.981, 't': 575.552, 'r': 504.003, 'b': 545.036, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 177]}]}, {'self_ref': '#/texts/87', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 112.981, 't': 534.855, 'r': 447.425, 'b': 526.157, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 70]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='References\\n- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\\n- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\\n- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\\n- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\\n- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/88', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 112.981, 't': 515.975, 'r': 483.898, 'b': 507.277, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 79]}]}, {'self_ref': '#/texts/89', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 112.981, 't': 497.096, 'r': 504.005, 'b': 444.762, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 440]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='References\\n- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index .\\n- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San JosÂ´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8_3 .'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/90', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 108.0, 't': 434.581, 'r': 504.003, 'b': 360.428, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 581]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='References\\n- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/91', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 108.0, 't': 350.247, 'r': 504.003, 'b': 308.822, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 288]}]}, {'self_ref': '#/texts/92', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 108.0, 't': 298.641, 'r': 504.003, 'b': 268.27, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 226]}]}, {'self_ref': '#/texts/93', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 108.0, 't': 257.943, 'r': 504.003, 'b': 238.48199999999997, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 164]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='References\\n- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\\n- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\\n- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/94', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 108.0, 't': 228.15499999999997, 'r': 504.001, 'b': 208.548, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 102]}]}, {'self_ref': '#/texts/95', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 108.0, 't': 198.36699999999996, 'r': 504.001, 'b': 178.76, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 109]}]}, {'self_ref': '#/texts/96', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 6, 'bbox': {'l': 108.0, 't': 168.57799999999997, 'r': 504.003, 'b': 149.11699999999996, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 128]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='References\\n- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\\n- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\\n- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/99', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 108.0, 't': 693.41, 'r': 463.754, 'b': 684.858, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 87]}]}, {'self_ref': '#/texts/595', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 329.602, 't': 428.537, 'r': 373.375, 'b': 423.963, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 14]}]}, {'self_ref': '#/texts/596', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 7, 'bbox': {'l': 108.0, 't': 419.051, 'r': 527.591, 'b': 377.771, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1026]}]}], 'headings': ['Appendix'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content=\"Appendix\\nIn this section, we illustrate a few examples of Docling's output in Markdown and JSON.\\n1 INTRODUCTION\\nDespite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\"),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/598', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 122.999, 't': 563.105, 'r': 338.603, 'b': 558.655, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 130]}]}, {'self_ref': '#/texts/599', 'parent': {'$ref': '#/tables/1'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 8, 'bbox': {'l': 122.872, 't': 552.103, 'r': 226.376, 'b': 509.485, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 489]}]}, {'self_ref': '#/tables/1', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/599'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 8, 'bbox': {'l': 125.37738800048828, 't': 505.5470886230469, 'r': 222.8895721435547, 'b': 437.7113037109375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['Appendix'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content=\"Appendix\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\n\\nTable 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\"),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/1', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/599'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 8, 'bbox': {'l': 125.37738800048828, 't': 505.5470886230469, 'r': 222.8895721435547, 'b': 437.7113037109375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['Appendix'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Appendix\\nCaption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, human = 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83. Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, MRCNN R50 R101 = 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5. Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, FRCNN R101 = 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4. Caption Footnote Formula List-item Page-footer Page-header Picture'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/1', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/599'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 8, 'bbox': {'l': 125.37738800048828, 't': 505.5470886230469, 'r': 222.8895721435547, 'b': 437.7113037109375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['Appendix'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Appendix\\nSection-header Table Text Title All, YOLO v5x6 = 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/600', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 122.884, 't': 431.161, 'r': 226.336, 'b': 341.547, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1252]}]}], 'headings': ['Appendix'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Appendix\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/602', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 122.865, 't': 327.581, 'r': 226.282, 'b': 284.81, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 584]}]}, {'self_ref': '#/texts/603', 'parent': {'$ref': '#/pictures/3'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 8, 'bbox': {'l': 235.911, 't': 469.973, 'r': 339.288, 'b': 441.408, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 322]}]}, {'self_ref': '#/texts/604', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 235.911, 't': 425.568, 'r': 338.603, 'b': 415.587, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 102]}]}], 'headings': ['5 EXPERIMENTS'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='5 EXPERIMENTS\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\nFigure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/605', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 235.776, 't': 416.2, 'r': 338.703, 'b': 382.797, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 397]}]}], 'headings': ['5 EXPERIMENTS'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='5 EXPERIMENTS\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/607', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 235.823, 't': 370.85, 'r': 338.7, 'b': 285.921, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1146]}]}], 'headings': ['Baselines for Object Detection'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Baselines for Object Detection\\nIn Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 Ã— 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/607', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 235.823, 't': 370.85, 'r': 338.7, 'b': 285.921, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1146]}]}, {'self_ref': '#/texts/608', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 366.3333333333333, 't': 563.0, 'r': 527.3333333333334, 'b': 545.6666666666666, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 484]}]}], 'headings': ['Baselines for Object Detection'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Baselines for Object Detection\\ndocument.\\nTable 2: Prediction perlormance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 beckbone were rained based on the network architecturesfrom the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utized was YOLOv5x6 [13]. All models were initialised using pre-trained weightsfrom the COCO 2017 dataset.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/2', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 8, 'bbox': {'l': 366.829833984375, 't': 543.0279541015625, 'r': 461.6680908203125, 'b': 450.7902526855469, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['Baselines for Object Detection'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Baselines for Object Detection\\nCaption, human = 84-89. Caption, MRCNN = 68.4. Caption, MRCNNFRCNN = 71.5. Caption,  = 70.1. Caption, YOLO = 77.7. Footnote, human = 83-91. Footnote, MRCNN = 70.9. Footnote, MRCNNFRCNN = 71.8. Footnote,  = 73.7. Footnote, YOLO = 77.2. Formula, human = 83-85. Formula, MRCNN = 60.1. Formula, MRCNNFRCNN = 63.4. Formula,  = 63.5. Formula, YOLO = 66.2. List-item, human = 89-48. List-item, MRCNN = 81.2. List-item, MRCNNFRCNN = 80.8. List-item,'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/2', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 8, 'bbox': {'l': 366.829833984375, 't': 543.0279541015625, 'r': 461.6680908203125, 'b': 450.7902526855469, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['Baselines for Object Detection'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Baselines for Object Detection\\n= 81.0. List-item, YOLO = 86.2. Page-ooer, human = 93-94. Page-ooer, MRCNN = 61.6. Page-ooer, MRCNNFRCNN = 59.3. Page-ooer,  = 58.9. Page-ooer, YOLO = 61.1. Page-header, human = 85-89. Page-header, MRCNN = 71.9. Page-header, MRCNNFRCNN = 70.0. Page-header,  = 72.0. Page-header, YOLO = 67.9. Picture, human = 69-71. Picture, MRCNN = 71.7. Picture, MRCNNFRCNN = 72.7. Picture,  = 72.0. Picture, YOLO = 77.1. Section-header, human = 83-84. Section-header, MRCNN = 67.6. Section-header, MRCNNFRCNN = 69.3. Section-header,'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/2', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 8, 'bbox': {'l': 366.829833984375, 't': 543.0279541015625, 'r': 461.6680908203125, 'b': 450.7902526855469, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['Baselines for Object Detection'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Baselines for Object Detection\\n= 68.4. Section-header, YOLO = 74.6. Table, human = 77-81. Table, MRCNN = 82.2. Table, MRCNNFRCNN = 82.9. Table,  = 82.2. Table, YOLO = 86.3. x1, human = 84-86. x1, MRCNN = 84.6. x1, MRCNNFRCNN = 85.8. x1,  = 85.4. x1, YOLO = 88.1. Title, human = 60-72. Title, MRCNN = 76.7. Title, MRCNNFRCNN = 80.4. Title,  = 79.9. Title, YOLO = 82.7. AlIl, human = 82-83. AlIl, MRCNN = 72.4. AlIl, MRCNNFRCNN = 73.5. AlIl,  = 73.4. AlIl, YOLO = 76.8'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/609', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 365.6666666666667, 't': 447.0, 'r': 530.6666666666666, 'b': 405.0, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1217]}]}], 'headings': ['Baselines for Object Detection'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Baselines for Object Detection\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human dcument-layout annotation. hird, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS -pxa fjeund je o sjoo-xe posopu oug punoe xoq-upunoq wnwu o o xoq umep-jesnne syuus fgeogewone joo uogeqouue based segments, which excludes only Table and Picture For the later, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cellsis that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/609', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 365.6666666666667, 't': 447.0, 'r': 530.6666666666666, 'b': 405.0, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1217]}]}], 'headings': ['Baselines for Object Detection'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='Baselines for Object Detection\\nmeasures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/611', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 366.0, 't': 391.33333333333337, 'r': 530.0, 'b': 369.66666666666663, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 583]}]}, {'self_ref': '#/texts/612', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 366.0, 't': 367.0, 'r': 528.6666666666666, 'b': 353.66666666666663, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 326]}]}, {'self_ref': '#/texts/613', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 366.3333333333333, 't': 351.33333333333337, 'r': 489.0, 'b': 347.0, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 101]}]}], 'headings': ['5EXPERIMENTS'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='5EXPERIMENTS\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthemore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\nFigure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNN network with ResNet50 backbone trained on increasing fractions of the DocLayNet dataset. The leaming curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with simlar data ill not yield significantly better predictions.\\npaper and leave the detaled evaluation of more reoent methods mentioned in Section 2 for future work.'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/614', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 365.6666666666667, 't': 344.33333333333337, 'r': 527.3333333333334, 'b': 331.0, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 354]}]}], 'headings': ['5EXPERIMENTS'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content=\"5EXPERIMENTS\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in 0 S'0 wo ofue pu sdeÎ¼ano o  (dvw) uospeud oene ueu Susn suogoped e po Agenb u aenje m om oeqnd 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API[16]\"),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/616', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 366.0, 't': 317.6666666666667, 'r': 529.3333333333334, 'b': 279.66666666666674, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1099]}]}], 'headings': ['BaselinesforObjectDetection'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content='BaselinesforObjectDetection\\nIn Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 Ã— 1025 pbxels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overallbetween 6 and 0 eug uogeopu poo6 e senj6 sL sobed popeoue-edjÎ¼ uo suogegoue ueunq esujed oug wo. pnduoo dvw oug ueg emo %01 DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixelupoau auou ou puey saugo au uo suogopeud sogeq ueqo o djou lou sep soxoq-ugpunoq wou'),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/616', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 366.0, 't': 317.6666666666667, 'r': 529.3333333333334, 'b': 279.66666666666674, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1099]}]}, {'self_ref': '#/texts/616', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 8, 'bbox': {'l': 366.0, 't': 317.6666666666667, 'r': 529.3333333333334, 'b': 279.66666666666674, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1099]}]}, {'self_ref': '#/texts/617', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 8, 'bbox': {'l': 108.0, 't': 266.424, 'r': 504.003, 'b': 225.14499999999998, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 393]}]}, {'self_ref': '#/texts/619', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 88.676, 't': 598.985, 'r': 186.95, 'b': 593.669, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 48]}]}, {'self_ref': '#/texts/620', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 190.471, 't': 598.985, 'r': 346.254, 'b': 593.669, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 81]}]}, {'self_ref': '#/texts/621', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 88.525, 't': 586.821, 'r': 346.401, 'b': 580.676, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 123]}]}], 'headings': ['BaselinesforObjectDetection'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content=\"BaselinesforObjectDetection\\npanuap uogeuoubes oteu poseq Yolov5x model does very welland even out-performs humans on selected labels such as Text, Table and Picture\\nThisis not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\\nFigure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\"),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/1086', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 88.676, 't': 581.225, 'r': 346.254, 'b': 575.08, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 124]}, {'page_no': 9, 'bbox': {'l': 88.507, 't': 306.868, 'r': 211.361, 'b': 300.541, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [125, 188]}]}, {'self_ref': '#/texts/1087', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 88.676, 't': 295.676, 'r': 211.36, 'b': 289.348, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 67]}, {'page_no': 9, 'bbox': {'l': 88.676, 't': 301.272, 'r': 211.36, 'b': 294.944, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [68, 136]}]}, {'self_ref': '#/texts/1088', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 88.507, 't': 290.08, 'r': 148.315, 'b': 283.752, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 30]}]}, {'self_ref': '#/texts/1089', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 223.4, 't': 309.117, 'r': 346.253, 'b': 302.789, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 62]}, {'page_no': 9, 'bbox': {'l': 223.57, 't': 297.925, 'r': 284.313, 'b': 291.597, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [63, 94]}]}, {'self_ref': '#/texts/1090', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 223.57, 't': 303.521, 'r': 346.254, 'b': 297.193, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 67]}]}, {'self_ref': '#/texts/1091', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 285.453, 't': 296.863, 'r': 301.055, 'b': 291.345, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 8]}, {'page_no': 9, 'bbox': {'l': 302.453, 't': 297.925, 'r': 313.455, 'b': 291.597, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [9, 15]}]}, {'self_ref': '#/texts/1092', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 314.595, 't': 296.863, 'r': 346.253, 'b': 291.345, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 19]}, {'page_no': 9, 'bbox': {'l': 223.57, 't': 286.733, 'r': 289.801, 'b': 280.405, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [20, 57]}]}, {'self_ref': '#/texts/1093', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 223.57, 't': 292.329, 'r': 346.256, 'b': 286.001, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 69]}]}, {'self_ref': '#/texts/1094', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 290.928, 't': 285.671, 'r': 303.416, 'b': 280.153, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 6]}]}, {'self_ref': '#/texts/1095', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 312.757, 't': 285.671, 'r': 330.984, 'b': 280.153, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 11]}]}], 'headings': ['BaselinesforObjectDetection'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content=\"BaselinesforObjectDetection\\nof row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric we distributed the annotation workload and performed continuous\\nonly. For phases three and four, a group of 40 dedicated annotators quality controls. Phase one and two required a small team of experts\\nwere assembled and supervised.\\nwhile coverage ensures that all meaningful items on a page can to a document category, such as\\nbe annotated. We refrained from class labels that are very specific\\nAbstract in the\\nScientific Articles semantics of the text. Labels such as\\ncategory. We also avoided class labels that are tightly linked to the\\nAuthor\\nAffiliation\"),\n",
              " Document(metadata={'source': 'https://arxiv.org/pdf/2408.09869', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/1096', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 9, 'bbox': {'l': 88.676, 't': 281.137, 'r': 504.003, 'b': 214.04200000000003, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 733]}]}, {'self_ref': '#/texts/1097', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 304.875, 't': 286.733, 'r': 311.628, 'b': 280.405, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 3]}]}, {'self_ref': '#/texts/1098', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 330.984, 't': 286.733, 'r': 346.253, 'b': 280.405, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 9]}]}, {'self_ref': '#/texts/1099', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 93.764, 't': 283.975, 'r': 176.983, 'b': 277.831, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 40]}]}, {'self_ref': '#/texts/1100', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 178.179, 't': 284.483, 'r': 212.133, 'b': 278.15599999999995, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 18]}]}], 'headings': ['BaselinesforObjectDetection'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 11465328351749295394, 'filename': '2408.09869v5.pdf'}}}, page_content=\"BaselinesforObjectDetection\\nteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\\nand\\n, as seen\\nPhase 1: Data selection and preparation.\\nOur inclusion cri-\")]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b332l-xmZp-R"
      },
      "source": [
        "Determining the splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bz6whDvWZp-S"
      },
      "outputs": [],
      "source": [
        "if EXPORT_TYPE == ExportType.DOC_CHUNKS:\n",
        "    splits = docs\n",
        "elif EXPORT_TYPE == ExportType.MARKDOWN:\n",
        "    from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "\n",
        "    splitter = MarkdownHeaderTextSplitter(\n",
        "        headers_to_split_on=[\n",
        "            (\"#\", \"Header_1\"),\n",
        "            (\"##\", \"Header_2\"),\n",
        "            (\"###\", \"Header_3\"),\n",
        "        ],\n",
        "    )\n",
        "    splits = [split for doc in docs for split in splitter.split_text(doc.page_content)]\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected export type: {EXPORT_TYPE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxUW1CHdZp-S"
      },
      "source": [
        "Inspecting some sample splits:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J8V9raUc5Mh",
        "outputId": "ed4a4369-e919-4c14-a768-fc04e459f873"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/2', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 138.755, 't': 724.9550146484376, 'r': 459.011, 'b': 712.6740146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 55]}]}, {'self_ref': '#/texts/3', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 149.327, 't': 710.8980146484375, 'r': 153.09, 'b': 704.0460146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1]}]}, {'self_ref': '#/texts/4', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 125.339, 't': 708.8980146484375, 'r': 472.929, 'b': 670.6310146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 189]}]}], 'headings': ['Precise Zero-Shot Dense Retrieval without Relevance Labels'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='Precise Zero-Shot Dense Retrieval without Relevance Labels\\nLuyu Gao âˆ— â€  Xueguang Ma âˆ— â€¡ Jimmy Lin â€¡ Jamie Callan â€ \\nâ€ \\nLanguage Technologies Institute, Carnegie Mellon University â€¡ David R. Cheriton School of Computer Science, University of Waterloo {luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/6', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 87.515, 't': 602.2010146484375, 'r': 273.866, 'b': 258.9040146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1286]}]}], 'headings': ['Abstract'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content=\"Abstract\\nWhile dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings ( HyDE ). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT ) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder (e.g. Contriever ) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search,\"), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/6', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 87.515, 't': 602.2010146484375, 'r': 273.866, 'b': 258.9040146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1286]}]}], 'headings': ['Abstract'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='Abstract\\nQA, fact verification) and languages (e.g. sw, ko, ja). 1'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/8', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 70.866, 't': 222.1640146484375, 'r': 290.949, 'b': 117.95401464843746, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 409]}]}, {'self_ref': '#/texts/9', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'footnote', 'prov': [{'page_no': 1, 'bbox': {'l': 87.006, 't': 109.5340146484375, 'r': 162.789, 'b': 99.78201464843755, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 21]}]}, {'self_ref': '#/texts/10', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'footnote', 'prov': [{'page_no': 1, 'bbox': {'l': 70.866, 't': 98.4910146484375, 'r': 292.272, 'b': 69.00601464843749, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 136]}]}, {'self_ref': '#/texts/11', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 306.142, 't': 625.4340146484375, 'r': 526.228, 'b': 575.4220146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 202]}]}], 'headings': ['1 Introduction'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='1 Introduction\\nDense retrieval (Lee et al., 2019; Karpukhin et al., 2020), the method of retrieving documents using semantic embedding similarities, has been shown successful across tasks like web search, question answering, and fact verification. A variety of methods such as negative mining (Xiong et al., 2021; Qu et al., 2021), distillation (Qu et al., 2021; Lin et al., 2021b; HofstÃ¤tter et al., 2021) and task-specific\\nâˆ— Equal contribution.\\n1 No models were trained or fine-tuned in making this preprint. Our open source code is available at https://github. com/texttron/hyde .\\npre-training (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) have been proposed to improve the effectiveness of supervised dense retrieval models.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/12', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 305.869, 't': 568.4880146484376, 'r': 526.225, 'b': 396.53301464843753, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 634]}]}], 'headings': ['1 Introduction'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='1 Introduction\\nOn the other hand, zero-shot dense retrieval still remains difficult. Many recent works consider the alternative transfer learning setup, where the dense retrievers are trained on a high-resource dataset and then evaluated on queries from new tasks. The MSMARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged querydocument pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in practice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO restricts commercial use and cannot be adopted in a variety of real-world search scenarios.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/13', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 305.749, 't': 389.60001464843754, 'r': 526.225, 'b': 68.60301464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1186]}]}], 'headings': ['1 Introduction'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='1 Introduction\\nIn this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and generalize across tasks. As supervision is not available, we start by examining self-supervised representation learning methods. Modern deep learning enables two distinct learning algorithms. At the token level, generative large language models (LLM) pretrained on large corpus have demonstrated strong natural language understanding (NLU) and generation (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse unseen instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/13', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 305.749, 't': 389.60001464843754, 'r': 526.225, 'b': 68.60301464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1186]}]}, {'self_ref': '#/texts/14', 'parent': {'$ref': '#/pictures/0'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 2, 'bbox': {'l': 70.507, 't': 639.7200146484375, 'r': 524.411, 'b': 619.2130146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 182]}]}, {'self_ref': '#/texts/74', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 70.866, 't': 593.9100146484375, 'r': 237.873, 'b': 584.5450146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 39]}]}], 'headings': ['1 Introduction'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='1 Introduction\\nal., 2022; Wei et al., 2022). Ouyang et al. (2022) show that with a small amount of data, GPT-3 (Brown et al., 2020) models can be aligned\\nFigure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries without changing the underlying GPT-3 and Contriever / mContriever models.\\nto human intent to follow instructions.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/75', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 70.353, 't': 580.0030146484376, 'r': 291.042, 'b': 204.81001464843757, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1309]}]}], 'headings': ['1 Introduction'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='1 Introduction\\nWith these ingredients, we propose to pivot through Hypothetical Document Embeddings ( HyDE ), and decompose dense retrieval into two tasks, a generative task performed by an instruction-following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to \"write a document that answers the question\", i.e. a hypothetical document. We expect the generative process to capture \"relevance\" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder\\'s dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner-product during contrastive training. Note that, interestingly, with HyDE factorization, the'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/75', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 70.353, 't': 580.0030146484376, 'r': 291.042, 'b': 204.81001464843757, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1309]}]}, {'self_ref': '#/texts/76', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 70.473, 't': 200.6930146484375, 'r': 290.946, 'b': 136.7070146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 209]}]}, {'self_ref': '#/texts/77', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 70.866, 't': 132.16501464843748, 'r': 290.949, 'b': 68.60301464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 244]}, {'page_no': 2, 'bbox': {'l': 305.749, 't': 593.9100146484375, 'r': 524.415, 'b': 557.4470146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [245, 366]}]}], 'headings': ['1 Introduction'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='1 Introduction\\nquery-document similarity score is no longer explicitly modeled nor computed. Instead, the retrieval task is cast into two NLU and NLG tasks.\\nHyDE appears unsupervised. No model is trained in HyDE : both the generative model and the contrastive encoder remain intact. Supervision signals were only involved in instruction learning of our backbone LLM.\\nIn our experiments, we show HyDE using InstructGPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) as backbone models significantly outperforms the previous state-of-the-art Contrieveronly zero-shot no-relevance system on 11 queries sets, covering tasks like Web Search, Question Answering, Fact Verification and languages like Swahili, Korean, Japanese.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/79', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 306.142, 't': 520.8770146484375, 'r': 526.217, 'b': 348.4960146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 627]}]}, {'self_ref': '#/texts/80', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 305.869, 't': 344.0290146484375, 'r': 524.793, 'b': 294.01701464843745, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 206]}]}], 'headings': ['2 Related Works'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='2 Related Works\\nDense Retrieval (Lee et al., 2019; Karpukhin et al., 2020) has been extensively studied after the emergence of pre-trained Transformer language models (Devlin et al., 2019). Researchers studied the metric learning problems, such as training loss (Karpukhin et al., 2020) and negative sampling (Xiong et al., 2021; Qu et al., 2021), and also introduced distillation (Qu et al., 2021; Lin et al., 2021b; HofstÃ¤tter et al., 2021). Later works studied the second stage pre-training of language model specifically for retrieval (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022).\\nThe popularity of dense retrieval can be partially attributed to the rich and successful research in very efficient minimum inner product search (MIPS) at very large (billion) scales (Johnson et al., 2017).'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/81', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 306.142, 't': 281.9140146484375, 'r': 525.317, 'b': 150.18101464843755, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 471]}]}, {'self_ref': '#/texts/82', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 2, 'bbox': {'l': 304.691, 't': 145.71401464843757, 'r': 524.789, 'b': 68.60301464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 287]}, {'page_no': 3, 'bbox': {'l': 70.866, 't': 767.1670146484375, 'r': 289.132, 'b': 744.2530146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [288, 345]}]}], 'headings': ['2 Related Works'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content=\"2 Related Works\\nInstructions-Following Language Models Soon after the emergence of LLMs, several groups of researchers discover that LLMs trained on data consisting of instructions and their execution can zero-shot generalize to perform new tasks with new instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). This can be done by standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022).\\nConcurrent to us, Asai et al. (2022) studied 'Task-aware Retrieval with Instructions'. They fine-tuned dense encoders that can also encode task-specific instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an instruction following generative LLM, as described above.\"), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/83', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 70.593, 't': 722.1400146484375, 'r': 290.948, 'b': 576.8580146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 486]}]}, {'self_ref': '#/texts/84', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 70.866, 't': 569.5010146484375, 'r': 289.317, 'b': 465.2920146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 394]}]}], 'headings': ['2 Related Works'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='2 Related Works\\nZero-Shot Dense Retrieval The tasks of zeroshot (dense) retrieval are arguably empirically defined by Thakur et al. (2021) for the neural retrieval community. Their BEIR benchmark consists of diverse retrieval tasks. The paper and many follow-up research generally consider the Transfer Learning setup where the dense retriever is first learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022).\\nHowever, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/85', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 70.593, 't': 457.9350146484375, 'r': 290.945, 'b': 394.3730146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 233]}]}, {'self_ref': '#/texts/86', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 70.528, 't': 372.26101464843754, 'r': 291.043, 'b': 159.23301464843757, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 762]}]}], 'headings': ['2 Related Works'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content=\"2 Related Works\\nBy the definition in Sachan et al. (2022), our setup can be roughly considered as 'unsupervised' . Strictly, as with Sachan et al. (2022), the only supervision resides in the LLM, in the processing of learning to follow instructions.\\nGenerative Retrieval Generative search is a new class of retrieval methods that use neural generative models as search indices (Metzler et al., 2021; Tay et al., 2022; Bevilacqua et al., 2022; Lee et al., 2022). These models use (constrained) decoding to generate document identifiers, such as id and sub-string, which map directly to real documents. They have to go through special training procedures over relevance data; effective search may also need to use novel forms of search indices (Bevilacqua et al., 2022; Lee et al., 2022). In comparison, our method uses the standard MIPS index and requires no training or training data. Our generative model produces an intermediate hypothetical document to be fed into a dense encoder, instead of a real document.\"), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/88', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 70.866, 't': 105.06701464843752, 'r': 290.941, 'b': 68.60301464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 142]}]}], 'headings': ['3 Methodology'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='3 Methodology\\nIn this section, we first formally define the problem of (zero-shot) dense retrieval. Then we will introduce how HyDE is designed to solve it.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/90', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 306.142, 't': 749.5540146484375, 'r': 526.221, 'b': 672.4430146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 273]}]}, {'self_ref': '#/texts/91', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 3, 'bbox': {'l': 314.835, 't': 657.4770146484375, 'r': 524.41, 'b': 646.6880146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 65]}]}, {'self_ref': '#/texts/92', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 306.142, 't': 632.7300146484375, 'r': 526.224, 'b': 540.8260146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 356]}]}, {'self_ref': '#/texts/93', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 305.869, 't': 537.4680146484375, 'r': 526.221, 'b': 446.8080146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 303]}]}], 'headings': ['3.1 Preliminaries'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='3.1 Preliminaries\\nDense retrieval models similarity between query and document with inner product similarity. Given a query q and document d , it uses two encoder function enc q and enc d to map them into d dimension vectors v q , v d , whose inner product is used as similarity measurement.\\n<!-- formula-not-decoded -->\\nFor zero-shot retrieval, we consider L query sets Q 1 , Q 2 , ..., Q L and their corresponding search corpus, document sets D 1 , D 2 , ..., D L . Denote the j -th query from i -th set query set Q i as q ij . We need to fully define mapping functions enc q and enc d without access to any query set Q i , document set D i , or any relevance judgment r ij .\\nThe difficulty of zero-shot dense retrieval lies precisely in Equation 1: it requires learning of two embedding functions (for query and document respectively) into the same embedding space where inner product captures relevance . Without relevance judgments/scores to fit, learning becomes intractable.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/95', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 306.142, 't': 415.9380146484375, 'r': 526.217, 'b': 311.72901464843744, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 351]}]}, {'self_ref': '#/texts/96', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 3, 'bbox': {'l': 373.9, 't': 296.72901464843744, 'r': 524.41, 'b': 285.97901464843744, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 22]}]}, {'self_ref': '#/texts/97', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 305.804, 't': 272.0160146484375, 'r': 526.222, 'b': 235.2030146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 136]}]}, {'self_ref': '#/texts/98', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 3, 'bbox': {'l': 322.906, 't': 220.23701464843748, 'r': 524.41, 'b': 209.44701464843752, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 45]}]}, {'self_ref': '#/texts/99', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 305.804, 't': 195.14101464843748, 'r': 524.597, 'b': 131.57901464843758, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 221]}]}, {'self_ref': '#/texts/100', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 3, 'bbox': {'l': 337.191, 't': 116.58001464843755, 'r': 524.41, 'b': 106.8660146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 42]}]}, {'self_ref': '#/texts/101', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 3, 'bbox': {'l': 306.142, 't': 91.8660146484375, 'r': 526.225, 'b': 68.60301464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 93]}]}], 'headings': ['3.2 HyDE'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='3.2 HyDE\\nHyDE circumvents the aforementioned learning problem by performing search in documentonly embedding space that captures documentdocument similarity. This can be easily learned using unsupervised contrastive learning (Izacard et al., 2021; Gao et al., 2021; Gao and Callan, 2022). We set document encoder enc d directly as a contrastive encoder enccon.\\n<!-- formula-not-decoded -->\\nThis function is also denoted as f for simplicity. This unsupervised contrastive encoder will be shared by all incoming document corpus.\\n<!-- formula-not-decoded -->\\nTo build the query vector, we consider in addition an instruction following LM, InstructLM. It takes a query q and a textual instruction INST and follows them to perform the task specified by INST. For simplicity, denote,\\n<!-- formula-not-decoded -->\\nNow we can use g to map queries to \"hypothetical\" documents by sampling from g , setting INST'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/102', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 70.866, 't': 767.1670146484375, 'r': 290.949, 'b': 608.7610146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 577]}]}, {'self_ref': '#/texts/103', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 70.353, 't': 604.5760146484375, 'r': 289.138, 'b': 581.6620146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 78]}]}, {'self_ref': '#/texts/104', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 4, 'bbox': {'l': 115.166, 't': 566.7630146484375, 'r': 289.134, 'b': 555.2860146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 48]}]}, {'self_ref': '#/texts/105', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 70.528, 't': 542.0830146484375, 'r': 291.039, 'b': 450.1270146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 369]}]}], 'headings': ['3.2 HyDE'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content=\"3.2 HyDE\\nto be 'write a paragraph that answers the question' . The generated document is not real, can and is likely to be ungrounded factually (Brown et al., 2020; Thoppilan et al., 2022). We only require it to capture relevance pattern. This is done by generating documents, i.e. providing examples. Critically, here we offload relevance modeling from representation learning model to an NLG model that generalizes significantly more easily, naturally, and effectively (Brown et al., 2020; Ouyang et al., 2022). Generating examples also replaces explicit modeling of relevance scores.\\nWe can now encode the generated document using the document encoder f . Write,\\n<!-- formula-not-decoded -->\\nFormally, g defines a probability distribution based on the chain rule. In this paper, we simply consider the expectation value, assuming the distribution of v q ij is uni-modal, i.e. the query is not ambiguous. The study of ambiguous queries and diversity is left to future work. We estimate Equation 5 by sampling N documents from g , [ Ë† d 1 , Ë† d 2 , ..., Ë† d N ] .\"), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/106', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 4, 'bbox': {'l': 114.094, 't': 438.9370146484375, 'r': 289.134, 'b': 406.1030146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 58]}]}, {'self_ref': '#/texts/107', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 4, 'bbox': {'l': 134.59, 't': 400.1030146484375, 'r': 289.134, 'b': 366.14801464843754, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 30]}]}, {'self_ref': '#/texts/108', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 70.353, 't': 354.1910146484375, 'r': 290.5, 'b': 344.82601464843754, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 51]}]}, {'self_ref': '#/texts/109', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 4, 'bbox': {'l': 103.476, 't': 331.3130146484375, 'r': 289.134, 'b': 297.3570146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 58]}]}, {'self_ref': '#/texts/110', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 70.866, 't': 290.0300146484375, 'r': 289.325, 'b': 171.77101464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 433]}]}], 'headings': ['3.2 HyDE'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='3.2 HyDE\\n<!-- formula-not-decoded -->\\n<!-- formula-not-decoded -->\\nWealso consider the query as a possible hypothesis,\\n<!-- formula-not-decoded -->\\nInner product is computed between Ë† v q ij and the set of all document vectors { f ( d ) | d âˆˆ D i } . The most similar documents are retrieved. Here the encoder function f serves as a lossy compressor that outputs dense vectors, where the extra details are filtered and left out from the vector. It further grounds the hypothetical vector to the actual corpus and the real documents. The full HyDE system is illustrated in Figure 1.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/113', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 70.866, 't': 119.04101464843757, 'r': 289.862, 'b': 68.60301464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 183]}, {'page_no': 4, 'bbox': {'l': 306.142, 't': 767.1670146484375, 'r': 526.224, 'b': 676.5070146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [184, 493]}]}], 'headings': ['4.1 Setup'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.1 Setup\\nImplementation We implement HyDE using InstructGPT , a GPT-3 model from the instruct series ( text-davinci-003 ; Ouyang et al. (2022)) and Contriever models (Izacard et al., 2021). We sample from InstructGPT using the OpenAI playground default temperature of 0.7 for open-ended generations. We use the English-only Contriever model for English retrieval tasks and multilingual mContriever for non-English tasks. We conducted retrieval experiments with the Pyserini toolkit (Lin et al., 2021a).'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/114', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 305.804, 't': 666.1180146484376, 'r': 525.772, 'b': 547.9340146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 403]}]}, {'self_ref': '#/texts/115', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 305.804, 't': 543.7500146484375, 'r': 526.322, 'b': 480.1880146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 227]}]}], 'headings': ['4.1 Setup'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.1 Setup\\nDatasets We consider web search query sets TREC DL19 (Craswell et al., 2020a) and DL20 (Craswell et al., 2020b); they are based on the MS-MARCO dataset (Bajaj et al., 2016). We also use a diverse collection of 6 low-resource datasets from the BEIR dataset (Thakur et al., 2021). For non-English retrieval, we consider Swahili, Korean, Japanese, and Bengali from the Mr.Tydi dataset (Zhang et al., 2021).\\nWe use different instructions for each dataset. They share a similar structure but have different quantifiers to control the exact form of the generated hypothetical documents. These instructions can be found in subsection A.1.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/116', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 305.804, 't': 469.8000146484375, 'r': 526.318, 'b': 351.6160146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 407]}]}], 'headings': ['4.1 Setup'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.1 Setup\\nCompared Systems Contriever models, Contriever and mContriever , serve as our major baseline. They are trained using unsupervised contrastive learning. HyDE retrievers share the exact same embedding spaces with them. The only difference is how the query vector is built. These comparisons allow us to easily examine the effect of HyDE . The classical heuristic-based lexical retriever BM25 is also included.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/117', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 306.142, 't': 347.43201464843753, 'r': 526.224, 'b': 134.82901464843758, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 741]}]}], 'headings': ['4.1 Setup'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.1 Setup\\nSeveral systems that involve fine-tuning on massive relevance data are also included as references. We consider models fine-tuned on MSMARCO and transferred, DPR and ANCE, from the BEIR paper. For multilingual, we include the mDPR model from Mr.Tydi paper and MSMARCO fine-tuned mBERT and XLM-R from the Contriever paper. We also include the state-ofthe-art transfer learning models: Contriever and mContriever fine-tuned on MS-MARCO, denoted Contriever FT and mContriever FT . These models have run through the state-of-the-art retrieval model training pipeline that involves second-stage retrieval-specific pre-training (Lee et al., 2019) and a few rounds of fine-tuning (Qu et al., 2021); they should be considered empirical upper bounds.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/119', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 4, 'bbox': {'l': 306.142, 't': 105.06701464843752, 'r': 524.594, 'b': 68.60301464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 139]}]}, {'self_ref': '#/texts/120', 'parent': {'$ref': '#/tables/0'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 5, 'bbox': {'l': 70.557, 't': 640.5260146484375, 'r': 524.413, 'b': 608.0630146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 219]}]}, {'self_ref': '#/tables/0', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/120'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 144.63929748535156, 't': 771.7276840209961, 'r': 450.0800476074219, 'b': 651.7601165771484, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4.2 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.2 Web Search\\nIn Table 1, we show retrieval results on TREC DL19 and TREC DL20. We see HyDE bring sizable improvements to Contriever across the board for\\n\\nTable 1: Results for web search on DL19/20. Best performing w/o relevance and overall system(s) are marked bold . DPR, ANCE and Contriever FT are in-domain supervised models that are finetuned on MS MARCO training data.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/0', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/120'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 144.63929748535156, 't': 771.7276840209961, 'r': 450.0800476074219, 'b': 651.7601165771484, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4.2 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.2 Web Search\\nw/o relevance judgement, DL19.map = w/o relevance judgement. w/o relevance judgement, DL19.ndcg@10 = w/o relevance judgement. w/o relevance judgement, DL19.recall@1k = . w/o relevance judgement, DL20.map = . w/o relevance judgement, DL20.ndcg@10 = . w/o relevance judgement, DL20.recall@1k = . BM25, DL19.map = 30.1. BM25, DL19.ndcg@10 = 50.6. BM25, DL19.recall@1k = 75.0. BM25, DL20.map = 28.6. BM25, DL20.ndcg@10 = 48.0. BM25, DL20.recall@1k = 78.6. Contriever, DL19.map = 24.0. Contriever, DL19.ndcg@10 = 44.5. Contriever, DL19.recall@1k = 74.6. Contriever, DL20.map = 24.0. Contriever,'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/0', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/120'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 144.63929748535156, 't': 771.7276840209961, 'r': 450.0800476074219, 'b': 651.7601165771484, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4.2 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.2 Web Search\\nDL20.ndcg@10 = 42.1. Contriever, DL20.recall@1k = 75.4. HyDE, DL19.map = 41.8. HyDE, DL19.ndcg@10 = 61.3. HyDE, DL19.recall@1k = 88.0. HyDE, DL20.map = 38.2. HyDE, DL20.ndcg@10 = 57.9. HyDE, DL20.recall@1k = 84.4. w/ relevance judgement, DL19.map = w/ relevance judgement. w/ relevance judgement, DL19.ndcg@10 = w/ relevance judgement. w/ relevance judgement, DL19.recall@1k = . w/ relevance judgement, DL20.map = . w/ relevance judgement, DL20.ndcg@10 = . w/ relevance judgement, DL20.recall@1k = . DPR, DL19.map = 36.5. DPR, DL19.ndcg@10 = 62.2. DPR, DL19.recall@1k = 76.9. DPR, DL20.map ='), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/0', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/120'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 144.63929748535156, 't': 771.7276840209961, 'r': 450.0800476074219, 'b': 651.7601165771484, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}, {'self_ref': '#/texts/121', 'parent': {'$ref': '#/tables/1'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 5, 'bbox': {'l': 75.149, 't': 362.6870146484375, 'r': 519.818, 'b': 353.7310146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 108]}]}, {'self_ref': '#/tables/1', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/121'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 131.70199584960938, 't': 595.8615264892578, 'r': 462.9095764160156, 'b': 373.6605224609375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4.2 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.2 Web Search\\n41.8. DPR, DL20.ndcg@10 = 65.3. DPR, DL20.recall@1k = 81.4. ANCE, DL19.map = 37.1. ANCE, DL19.ndcg@10 = 64.5. ANCE, DL19.recall@1k = 75.5. ANCE, DL20.map = 40.8. ANCE, DL20.ndcg@10 = 64.6. ANCE, DL20.recall@1k = 77.6. Contriever FT, DL19.map = 41.7. Contriever FT, DL19.ndcg@10 = 62.1. Contriever FT, DL19.recall@1k = 83.6. Contriever FT, DL20.map = 43.6. Contriever FT, DL20.ndcg@10 = 63.2. Contriever FT, DL20.recall@1k = 85.8\\n\\nTable 2: Low resource tasks from BEIR. Best performing w/o relevance and overall system(s) are marked bold .'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/1', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/121'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 131.70199584960938, 't': 595.8615264892578, 'r': 462.9095764160156, 'b': 373.6605224609375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4.2 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.2 Web Search\\nnDCG@10, Scifact = nDCG@10. nDCG@10, Arguana = nDCG@10. nDCG@10, Trec-Covid = nDCG@10. nDCG@10, FiQA = nDCG@10. nDCG@10, DBPedia = nDCG@10. nDCG@10, TREC-NEWS = nDCG@10. w/o relevance judgement, Scifact = w/o relevance judgement. w/o relevance judgement, Arguana = w/o relevance judgement. w/o relevance judgement, Trec-Covid = w/o relevance judgement. w/o relevance judgement, FiQA = w/o relevance judgement. w/o relevance judgement, DBPedia = w/o relevance judgement. w/o relevance judgement, TREC-NEWS = w/o relevance judgement. BM25, Scifact = 67.9. BM25, Arguana = 39.7. BM25, Trec-Covid = 59.5. BM25, FiQA = 23.6. BM25,'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/1', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/121'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 131.70199584960938, 't': 595.8615264892578, 'r': 462.9095764160156, 'b': 373.6605224609375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4.2 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.2 Web Search\\nDBPedia = 31.8. BM25, TREC-NEWS = 39.5. Contriever, Scifact = 64.9. Contriever, Arguana = 37.9. Contriever, Trec-Covid = 27.3. Contriever, FiQA = 24.5. Contriever, DBPedia = 29.2. Contriever, TREC-NEWS = 34.8. HyDE, Scifact = 69.1. HyDE, Arguana = 46.6. HyDE, Trec-Covid = 59.3. HyDE, FiQA = 27.3. HyDE, DBPedia = 36.8. HyDE, TREC-NEWS = 44.0. w/ relevance judgement, Scifact = w/ relevance judgement. w/ relevance judgement, Arguana = w/ relevance judgement. w/ relevance judgement, Trec-Covid = w/ relevance judgement. w/ relevance judgement, FiQA = w/ relevance judgement. w/ relevance judgement, DBPedia = w/ relevance judgement. w/ relevance judgement, TREC-NEWS = w/ relevance judgement. DPR, Scifact ='), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/1', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/121'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 131.70199584960938, 't': 595.8615264892578, 'r': 462.9095764160156, 'b': 373.6605224609375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4.2 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.2 Web Search\\n31.8. DPR, Arguana = 17.5. DPR, Trec-Covid = 33.2. DPR, FiQA = 29.5. DPR, DBPedia = 26.3. DPR, TREC-NEWS = 16.1. ANCE, Scifact = 50.7. ANCE, Arguana = 41.5. ANCE, Trec-Covid = 65.4. ANCE, FiQA = 30.0. ANCE, DBPedia = 28.1. ANCE, TREC-NEWS = 38.2. Contriever FT, Scifact = 67.7. Contriever FT, Arguana = 44.6. Contriever FT, Trec-Covid = 59.6. Contriever FT, FiQA = 32.9. Contriever FT, DBPedia = 41.3. Contriever FT, TREC-NEWS = 42.8. Recall@100, Scifact = Recall@100. Recall@100, Arguana = Recall@100. Recall@100, Trec-Covid = Recall@100. Recall@100,'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/1', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/121'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 131.70199584960938, 't': 595.8615264892578, 'r': 462.9095764160156, 'b': 373.6605224609375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4.2 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.2 Web Search\\nFiQA = Recall@100. Recall@100, DBPedia = Recall@100. Recall@100, TREC-NEWS = Recall@100. w/o relevance judgement, Scifact = w/o relevance judgement. w/o relevance judgement, Arguana = w/o relevance judgement. w/o relevance judgement, Trec-Covid = w/o relevance judgement. w/o relevance judgement, FiQA = w/o relevance judgement. w/o relevance judgement, DBPedia = w/o relevance judgement. w/o relevance judgement, TREC-NEWS = w/o relevance judgement. BM25, Scifact = 92.5. BM25, Arguana = 93.2. BM25, Trec-Covid = 49.8. BM25, FiQA = 54.0. BM25, DBPedia = 46.8. BM25, TREC-NEWS = 44.7. Contriever, Scifact = 92.6. Contriever, Arguana = 90.1. Contriever, Trec-Covid = 17.2. Contriever, FiQA ='), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/1', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/121'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 131.70199584960938, 't': 595.8615264892578, 'r': 462.9095764160156, 'b': 373.6605224609375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4.2 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.2 Web Search\\n56.2. Contriever, DBPedia = 45.3. Contriever, TREC-NEWS = 42.3. HyDE, Scifact = 96.4. HyDE, Arguana = 97.9. HyDE, Trec-Covid = 41.4. HyDE, FiQA = 62.1. HyDE, DBPedia = 47.2. HyDE, TREC-NEWS = 50.9. w/ relevance judgement, Scifact = w/ relevance judgement. w/ relevance judgement, Arguana = w/ relevance judgement. w/ relevance judgement, Trec-Covid = w/ relevance judgement. w/ relevance judgement, FiQA = w/ relevance judgement. w/ relevance judgement, DBPedia = w/ relevance judgement. w/ relevance judgement, TREC-NEWS = w/ relevance judgement. DPR, Scifact = 72.7. DPR, Arguana = 75.1. DPR, Trec-Covid = 21.2. DPR, FiQA = 34.2. DPR, DBPedia = 34.9. DPR, TREC-NEWS = 21.5. ANCE, Scifact'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/1', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/121'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 5, 'bbox': {'l': 131.70199584960938, 't': 595.8615264892578, 'r': 462.9095764160156, 'b': 373.6605224609375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}, {'self_ref': '#/texts/122', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 70.866, 't': 328.44501464843756, 'r': 289.136, 'b': 278.43201464843753, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 159]}]}], 'headings': ['4.2 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.2 Web Search\\n= 81.6. ANCE, Arguana = 93.7. ANCE, Trec-Covid = 45.7. ANCE, FiQA = 58.1. ANCE, DBPedia = 31.9. ANCE, TREC-NEWS = 39.8. Contriever FT, Scifact = 94.7. Contriever FT, Arguana = 97.7. Contriever FT, Trec-Covid = 40.7. Contriever FT, FiQA = 65.6. Contriever FT, DBPedia = 54.1. Contriever FT, TREC-NEWS = 49.2\\nboth precision-oriented and recall metrics. While unsupervised Contriever can underperform the classical BM25 approach, HyDE outperforms BM25 by large margins.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/123', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 70.866, 't': 227.00901464843753, 'r': 290.949, 'b': 68.60301464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 519]}]}], 'headings': ['4.2 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.2 Web Search\\nHyDE remains competitive even when compared to fine-tuned models. Note that TREC DL19/20 are search tasks defined on MS-MARCO and there, all the fine-tuned models are richly supervised . On TREC DL19, HyDE shows comparable map and ndcg@10 to Contriever FT and best recall@1k. On DL20, HyDE gets around 10% lower map and ndcg@10 than Contriever FT and similar recall@1k. The ANCE model shows better ndcg@10 numbers than HyDE but lower recall, suggesting it may be biased to a subset of queries and/or relevant documents.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/125', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 306.142, 't': 309.1970146484375, 'r': 526.217, 'b': 204.98701464843748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 358]}]}, {'self_ref': '#/texts/126', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 305.749, 't': 199.91101464843746, 'r': 526.318, 'b': 68.60301464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 453]}]}, {'self_ref': '#/texts/127', 'parent': {'$ref': '#/tables/2'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 6, 'bbox': {'l': 70.557, 't': 652.7590146484375, 'r': 289.136, 'b': 632.2360146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 98]}]}, {'self_ref': '#/tables/2', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/127'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 6, 'bbox': {'l': 79.23062896728516, 't': 772.2752075195312, 'r': 282.475341796875, 'b': 663.8870391845703, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4.3 Low Resource Retrieval'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.3 Low Resource Retrieval\\nIn Table 2, we show retrieval results on lowresource tasks from BEIR. Similar to web search, HyDE again brings sizable improvements to Contriever across the board in terms of both ndcg and recall. HyDE is only outperformed by BM25 on one dataset, TREC-Covid but with a tiny 0.2 margin; in comparison, the underlying Contriever underperforms by more than 50%.\\nWe also observe HyDE demonstrates strong performance compared to fine-tuned models. HyDE generally shows better performance than ANCE and DPR, even though the two are fine-tuned on MS-MARCO and ANCE also involves some sophisticated hard negative techniques. Contriever FT shows performance advantages on FiQA and DBPedia. These involve retrieval of financial posts or entities respectively. We believe the performance difference can be attributed to the\\n\\nTable 3: MRR@100 on Mr.Tydi. Best performing w/o relevance and overall system(s) are marked bold .'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/2', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/127'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 6, 'bbox': {'l': 79.23062896728516, 't': 772.2752075195312, 'r': 282.475341796875, 'b': 663.8870391845703, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['4.3 Low Resource Retrieval'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.3 Low Resource Retrieval\\nw/o relevance judgement, Swahili = w/o relevance judgement. w/o relevance judgement, Korean = . w/o relevance judgement, Japanese = . w/o relevance judgement, Bengali = . BM25, Swahili = 38.9. BM25, Korean = 28.5. BM25, Japanese = 21.2. BM25, Bengali = 41.8. mContriever, Swahili = 38.3. mContriever, Korean = 22.3. mContriever, Japanese = 19.5. mContriever, Bengali = 35.3. HyDE, Swahili = 41.7. HyDE, Korean = 30.6. HyDE, Japanese = 30.7. HyDE, Bengali = 41.3. w/ relevance judgement, Swahili = w/ relevance judgement. w/ relevance judgement, Korean = . w/ relevance judgement, Japanese = . w/ relevance judgement, Bengali = . mDPR, Swahili = 7.3. mDPR, Korean = 21.9. mDPR, Japanese = 18.1. mDPR, Bengali = 25.8. mBERT, Swahili ='), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/2', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/127'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 6, 'bbox': {'l': 79.23062896728516, 't': 772.2752075195312, 'r': 282.475341796875, 'b': 663.8870391845703, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}, {'self_ref': '#/texts/128', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 6, 'bbox': {'l': 70.866, 't': 608.5390146484375, 'r': 290.95, 'b': 585.6250146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 79]}]}], 'headings': ['4.3 Low Resource Retrieval'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.3 Low Resource Retrieval\\n37.4. mBERT, Korean = 28.1. mBERT, Japanese = 27.1. mBERT, Bengali = 35.1. XLM-R, Swahili = 35.1. XLM-R, Korean = 32.2. XLM-R, Japanese = 24.8. XLM-R, Bengali = 41.7. mContriever FT, Swahili = 51.2. mContriever FT, Korean = 34.2. mContriever FT, Japanese = 32.4. mContriever FT, Bengali = 42.3\\nunder-specification of the instruction; more elaborative instructions may help.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/130', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 6, 'bbox': {'l': 70.866, 't': 555.5200146484375, 'r': 291.048, 'b': 451.3100146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 380]}]}, {'self_ref': '#/texts/131', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 6, 'bbox': {'l': 70.473, 't': 447.1260146484375, 'r': 290.941, 'b': 315.8180146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 466]}]}], 'headings': ['4.4 Multilingual Retrieval'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='4.4 Multilingual Retrieval\\nMultilingual setup poses several additional challenges to HyDE . The small-sized contrastive encoder gets saturated as the number of languages scales (Conneau et al., 2020; Izacard et al., 2021). Meanwhile, our generative LLM faces an opposite issue: with languages of not as high resource as English or French, the high capacity LLM can get under-trained (Hoffmann et al., 2022).\\nNevertheless, in Table 3, we still find HyDE able to improve the mContriever model. It can outperform non-Contriever models fine-tuned on and transferred from MS-MARCO. On the other hand, we do observe some margins between HyDE and fine-tuned mContriever FT . Since HyDE and mContriever FT use similar contrastive encoders, we hypothesize this is because the non-English languages we considered are under-trained in both pre-training and instruction learning stages.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/133', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 6, 'bbox': {'l': 70.528, 't': 281.2880146484375, 'r': 290.941, 'b': 204.17701464843753, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 263]}]}], 'headings': ['5 Analysis'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='5 Analysis\\nThe generative LLM and contrastive encoder make up the backbone of HyDE . In this section, we study the effect of changing their realizations. In particular, we consider smaller language models (LM) and fine-tuned encoders. We conduct our studies on TREC DL19/20.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/135', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 6, 'bbox': {'l': 70.048, 't': 174.07201464843752, 'r': 289.315, 'b': 96.96101464843753, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 253]}]}, {'self_ref': '#/texts/136', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'footnote', 'prov': [{'page_no': 6, 'bbox': {'l': 70.866, 't': 88.52901464843751, 'r': 290.031, 'b': 69.00601464843749, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 69]}]}, {'self_ref': '#/texts/137', 'parent': {'$ref': '#/tables/3'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 6, 'bbox': {'l': 305.833, 't': 619.8740146484375, 'r': 526.066, 'b': 575.4400146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 163]}]}, {'self_ref': '#/tables/3', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/137'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 6, 'bbox': {'l': 338.0185546875, 't': 772.8914184570312, 'r': 491.46368408203125, 'b': 631.3646240234375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['5.1 Effect of Different Generative Models'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='5.1 Effect of Different Generative Models\\nIn Table 4, we show HyDE using other instruction-following language models. In particular, we consider a 52-billion Cohere model ( command-xlarge-20221108 ) and a 11-billion FLAN model ( FLAN-T5-xxl ; Wei et al. (2022)). 2 Generally, we observe that all\\n2 Model sizes are from https://crfm.stanford.edu/ helm/v1.0/?models .\\n\\nTable 4: NDCG@10 on TREC DL19/20. Effect of changing different instruction LMs and using finetuned encoder. Best w/o relevance and overall models are marked bold .'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/3', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/137'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 6, 'bbox': {'l': 338.0185546875, 't': 772.8914184570312, 'r': 491.46368408203125, 'b': 631.3646240234375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}], 'headings': ['5.1 Effect of Different Generative Models'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='5.1 Effect of Different Generative Models\\nContriever, DL19 = 44.5. Contriever, DL20 = 42.1. Contriever FT, DL19 = 62.1. Contriever FT, DL20 = 63.2. HyDE, DL19 = . HyDE, DL20 = . w/ Contriever, DL19 = . w/ Contriever, DL20 = . w/ Flan-T5 (11b), DL19 = 48.9. w/ Flan-T5 (11b), DL20 = 52.9. w/ Cohere (52b), DL19 = 53.8. w/ Cohere (52b), DL20 = 53.8. w/ GPT (175b), DL19 = 61.3. w/ GPT (175b), DL20 = 57.9. w/ Contriever FT, DL19 = . w/ Contriever FT, DL20 = . w/ Flan-T5 (11b), DL19 = 60.2. w/ Flan-T5 (11b), DL20 = 62.1. w/ Cohere'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/tables/3', 'parent': {'$ref': '#/body'}, 'children': [{'$ref': '#/texts/137'}], 'content_layer': 'body', 'label': 'table', 'prov': [{'page_no': 6, 'bbox': {'l': 338.0185546875, 't': 772.8914184570312, 'r': 491.46368408203125, 'b': 631.3646240234375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 0]}]}, {'self_ref': '#/texts/138', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 6, 'bbox': {'l': 305.749, 't': 548.9190146484375, 'r': 524.79, 'b': 444.7090146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 339]}]}], 'headings': ['5.1 Effect of Different Generative Models'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='5.1 Effect of Different Generative Models\\n(52b), DL19 = 61.4. w/ Cohere (52b), DL20 = 63.1. w/ GPT (175b), DL19 = 67.4. w/ GPT (175b), DL20 = 63.5\\nmodels bring improvement to the unsupervised Contriever , with larger models bringing larger improvements. At the time when this paper is written, the Cohere model is still experimental without much detail disclosed. We can only tentatively hypothesize that training techniques may have also played some role in the performance difference.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/140', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 6, 'bbox': {'l': 305.804, 't': 410.8480146484375, 'r': 526.321, 'b': 198.24501464843752, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 757]}]}], 'headings': ['5.2 HyDE with Fine-tuned Encoder'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='5.2 HyDE with Fine-tuned Encoder\\nTo begin with, HyDE with fine-tuned encoder is not the intended usage: HyDE is more powerful and irreplaceable when few relevance labels are present. Here we are interested to find out if and how HyDE embedding can affect fine-tuned encoders. In Table 4, we see that less powerful instruction LMs can negatively impact the overall performance of the fine-tuned retriever. (To remind our readers, Contriever FT is in-domain supervisedly fine-tuned for TREC DL19/20). The performance degradations remain small. On the other hand, we also observe the InstructGPT model able to further bring up the performance, especially on DL19. This suggests that there may still exist certain factors not captured by the fine-tuned encoder but only by the generative model.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/142', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 6, 'bbox': {'l': 305.749, 't': 159.26301464843755, 'r': 526.319, 'b': 68.60301464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 349]}, {'page_no': 7, 'bbox': {'l': 70.473, 't': 767.1670146484375, 'r': 290.949, 'b': 595.2110146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [350, 999]}]}], 'headings': ['6 Conclusion'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='6 Conclusion\\nAt the end of the paper, we encourage the readers to take a moment and reflect on the HyDE model. Compare it to some of the other recently seen retrievers or re-ranker. These other models probably differ in their architecture, training method, and/or task, but probably all of them involve modeling relevance scores between a pair of query and docu- ment. Dense retrievers consider vector similarities while self-attentive re-rankers regression scores. In comparison, the concept of relevance in HyDE is captured by an NLG model and the language generation process. We demonstrate in many cases, HyDE can be as effective as dense retrievers that learn to model numerical relevance scores. So, is numerical relevance just a statistical artifact of language understanding? Will a weak retriever theoretically suffice as the NLU & NLG models rapidly become stronger? Rushing to conclusions is not smart; more works need to be done to get answers. With this paper, we just want to raise these questions.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/143', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 70.593, 't': 589.9650146484375, 'r': 290.941, 'b': 458.6570146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 441]}]}, {'self_ref': '#/texts/144', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 70.473, 't': 453.4110146484375, 'r': 290.95, 'b': 308.5540146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 508]}]}], 'headings': ['6 Conclusion'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='6 Conclusion\\nConcretely in this paper, we introduce a new paradigm of interactions between LLM and dense encoder/retriever. We demonstrate (part of) relevance modeling and instruction understanding can be delegated to the more powerful and flexible LLM. As a consequence, the need for relevance labels is removed. We are excited to see how this can be generalized further to more sophisticated tasks like multi-hop retrieval/QA and conversational search.\\nWeargue HyDE is also of practical use though not necessarily over the entire lifespan of a search system. At the very beginning of the life of the search system, serving queries using HyDE offers performance comparable to a fine-tuned model, which no other relevance-free model can offer. As the search log grows, a supervised dense retriever can be gradually rolled out. As the dense retriever grows stronger, more queries will be routed to it, with only less common and emerging ones going to HyDE backend.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/146', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 70.866, 't': 257.2070146484375, 'r': 290.79, 'b': 215.7790146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 170]}]}, {'self_ref': '#/texts/147', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 70.866, 't': 200.15601464843758, 'r': 290.79, 'b': 136.81001464843757, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 283]}]}, {'self_ref': '#/texts/148', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 70.866, 't': 121.1880146484375, 'r': 290.79, 'b': 68.80001464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 211]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. Task-aware retrieval with instructions.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2016. Ms marco: A human generated machine reading comprehension dataset.\\nMichele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive search engines: Generating substrings as document identifiers. CoRR , abs/2204.10628.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/149', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 306.142, 't': 766.5510146484376, 'r': 526.066, 'b': 604.5740146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 694]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/150', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 306.142, 't': 591.6120146484375, 'r': 526.156, 'b': 374.8410146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 950]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/150', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 306.142, 't': 591.6120146484375, 'r': 526.156, 'b': 374.8410146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 950]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nWojciech Zaremba. 2021. Evaluating large language models trained on code.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/151', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 306.142, 't': 361.8800146484375, 'r': 526.156, 'b': 112.23201464843748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1071]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child,'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/151', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 306.142, 't': 361.8800146484375, 'r': 526.156, 'b': 112.23201464843748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1071]}]}, {'self_ref': '#/texts/152', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 7, 'bbox': {'l': 306.142, 't': 99.2700146484375, 'r': 526.066, 'b': 68.80001464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 142]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nOleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettle-'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/153', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 81.466, 't': 766.5510146484376, 'r': 290.79, 'b': 703.2040146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 255]}]}, {'self_ref': '#/texts/154', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 70.866, 't': 691.3600146484375, 'r': 289.495, 'b': 660.8900146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 135]}]}, {'self_ref': '#/texts/155', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 70.866, 't': 649.0460146484376, 'r': 290.88, 'b': 607.6170146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 168]}]}, {'self_ref': '#/texts/156', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 70.866, 't': 595.7730146484375, 'r': 290.79, 'b': 499.5490146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 419]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\n- moyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 84408451, Online. Association for Computational Linguistics.\\n- Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2020a. Overview of the trec 2019 deep learning track.\\n- Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. Voorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820.\\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/157', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 70.866, 't': 487.7050146484375, 'r': 290.79, 'b': 424.3580146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 284]}]}, {'self_ref': '#/texts/158', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 70.866, 't': 412.5140146484375, 'r': 290.791, 'b': 338.2090146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 309]}]}, {'self_ref': '#/texts/159', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 70.866, 't': 326.36401464843743, 'r': 290.88, 'b': 252.05901464843748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 303]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\n- Luyu Gao and Jamie Callan. 2021. Condenser: a pretraining architecture for dense retrieval. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 981-993, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n- Luyu Gao and Jamie Callan. 2022. Unsupervised corpus aware language model pre-training for dense passage retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2843-2853, Dublin, Ireland. Association for Computational Linguistics.\\n- Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/160', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 70.866, 't': 240.21401464843757, 'r': 290.79, 'b': 143.9910146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 412]}]}, {'self_ref': '#/texts/161', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 70.866, 't': 132.14701464843756, 'r': 290.79, 'b': 68.80001464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 294]}]}, {'self_ref': '#/texts/162', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 317.051, 't': 766.5510146484376, 'r': 526.066, 'b': 747.0400146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 80]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content=\"References\\n- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models.\\n- Sebastian HofstÃ¤tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval ,\\n- SIGIR '21, page 113-122, New York, NY, USA. Association for Computing Machinery.\"), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/163', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 306.142, 't': 735.9870146484375, 'r': 526.156, 'b': 683.5990146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 223]}]}, {'self_ref': '#/texts/164', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 306.142, 't': 672.5470146484375, 'r': 526.156, 'b': 642.0770146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 118]}]}, {'self_ref': '#/texts/165', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 306.142, 't': 631.0240146484375, 'r': 526.066, 'b': 545.7600146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 346]}]}, {'self_ref': '#/texts/166', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 306.142, 't': 534.7070146484375, 'r': 524.413, 'b': 515.1960146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 90]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\n- Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. CoRR , abs/2112.09118.\\n- Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2017. Billion-scale similarity search with gpus. CoRR , abs/1702.08734.\\n- Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 67696781, Online. Association for Computational Linguistics.\\n- Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. 2022. Generative multi-hop retrieval.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/167', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 306.142, 't': 504.1430146484375, 'r': 526.156, 'b': 440.79701464843754, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 298]}]}, {'self_ref': '#/texts/168', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 306.142, 't': 429.74401464843754, 'r': 526.156, 'b': 344.48001464843753, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 368]}]}, {'self_ref': '#/texts/169', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 306.142, 't': 333.42701464843753, 'r': 526.156, 'b': 259.1210146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 305]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\n- Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6086-6096, Florence, Italy. Association for Computational Linguistics.\\n- Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021a. Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021) , pages 2356-2362.\\n- Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021b. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021) , pages 163173, Online. Association for Computational Linguistics.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/170', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 306.142, 't': 248.06901464843747, 'r': 526.066, 'b': 217.59901464843756, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 136]}]}, {'self_ref': '#/texts/171', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 306.142, 't': 206.54601464843745, 'r': 526.066, 'b': 110.32301464843749, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 414]}]}, {'self_ref': '#/texts/172', 'parent': {'$ref': '#/groups/0'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 8, 'bbox': {'l': 306.142, 't': 99.2700146484375, 'r': 526.156, 'b': 68.80001464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 151]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\n- Zheng Liu and Yingxia Shao. 2022. Retromae: Pretraining retrieval-oriented transformers via masked auto-encoder. ArXiv , abs/2205.12035.\\n- Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold Overwijk. 2021. Less is more: Pretrain a strong Siamese encoder for dense text retrieval using a weak decoder. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 2780-2791, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n- Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 2021. Rethinking search: making domain experts out of dilettantes. SIGIR Forum , 55(1):13:1-13:27.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/173', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 70.866, 't': 766.5510146484376, 'r': 290.88, 'b': 692.2450146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 342]}]}, {'self_ref': '#/texts/174', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 70.866, 't': 676.4430146484375, 'r': 290.79, 'b': 591.1790146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 363]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2791-2809, Seattle, United States. Association for Computational Linguistics.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/175', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 70.866, 't': 575.3760146484375, 'r': 290.79, 'b': 468.19401464843753, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 437]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 5835-5847, Online. Association for Computational Linguistics.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/176', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 70.866, 't': 452.3920146484375, 'r': 290.791, 'b': 126.03101464843758, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1373]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen,'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/176', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 70.866, 't': 452.3920146484375, 'r': 290.791, 'b': 126.03101464843758, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1373]}]}, {'self_ref': '#/texts/177', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 70.866, 't': 110.22901464843756, 'r': 290.79, 'b': 68.80001464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 186]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content=\"References\\nZhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis & insights from training gopher.\\nDevendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage retrieval with zero-shot question generation.\"), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/178', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 306.142, 't': 766.5510146484376, 'r': 526.153, 'b': 571.6970146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 812]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault FÃ©vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/179', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 306.142, 't': 560.7660146484375, 'r': 526.156, 'b': 497.4190146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 257]}]}, {'self_ref': '#/texts/180', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 306.142, 't': 486.4880146484375, 'r': 526.066, 'b': 434.10001464843754, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 203]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nYi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Prakash Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022. Transformer memory as a differentiable search index. CoRR , abs/2202.06991.\\nNandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. CoRR , abs/2104.08663.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/181', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 306.142, 't': 423.1690146484375, 'r': 526.066, 'b': 195.43901464843748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 949]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil,'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/181', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 306.142, 't': 423.1690146484375, 'r': 526.066, 'b': 195.43901464843748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 949]}]}, {'self_ref': '#/texts/182', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 306.142, 't': 184.50701464843746, 'r': 526.155, 'b': 99.24301464843757, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 385]}]}, {'self_ref': '#/texts/183', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 9, 'bbox': {'l': 306.142, 't': 88.31101464843755, 'r': 526.066, 'b': 68.80001464843747, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 94]}, {'page_no': 10, 'bbox': {'l': 81.606, 't': 766.5510146484376, 'r': 290.878, 'b': 714.1630146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [95, 308]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\nBlaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications. CoRR , abs/2201.08239.\\nKexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022. GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2345-2360, Seattle, United States. Association for Computational Linguistics.\\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net.'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/184', 'parent': {'$ref': '#/groups/1'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 10, 'bbox': {'l': 70.866, 't': 702.7900146484375, 'r': 290.88, 'b': 628.4840146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 331]}]}, {'self_ref': '#/texts/185', 'parent': {'$ref': '#/groups/1'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 10, 'bbox': {'l': 70.866, 't': 617.1110146484375, 'r': 290.791, 'b': 553.7650146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 284]}]}, {'self_ref': '#/texts/186', 'parent': {'$ref': '#/groups/1'}, 'children': [], 'content_layer': 'body', 'label': 'list_item', 'prov': [{'page_no': 10, 'bbox': {'l': 70.866, 't': 542.3910146484375, 'r': 290.88, 'b': 511.9210146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 132]}]}], 'headings': ['References'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='References\\n- Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net.\\n- Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and Arnold Overwijk. 2022. Coco-dr: Combating distribution shifts in zero-shot dense retrieval with contrastive and distributionally robust learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing .\\n- Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021. Mr. TyDi: A multi-lingual benchmark for dense retrieval. arXiv:2108.08787 .'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/190', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 708.4830146484375, 'r': 284.294, 'b': 699.1180146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 45]}]}, {'self_ref': '#/texts/191', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 694.9340146484375, 'r': 193.955, 'b': 685.5690146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 20]}]}, {'self_ref': '#/texts/192', 'parent': {'$ref': '#/groups/2'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 681.3850146484375, 'r': 123.864, 'b': 672.0200146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 8]}]}], 'headings': ['A.1.1 Web Search'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='A.1.1 Web Search\\nPlease write a passage to answer the question\\nQuestion: [QUESTION]\\nPassage:'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/194', 'parent': {'$ref': '#/groups/3'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 625.5200146484375, 'r': 370.945, 'b': 616.1550146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 67]}]}, {'self_ref': '#/texts/195', 'parent': {'$ref': '#/groups/3'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 611.9710146484375, 'r': 153.483, 'b': 602.6060146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 14]}]}, {'self_ref': '#/texts/196', 'parent': {'$ref': '#/groups/3'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 598.4220146484375, 'r': 123.864, 'b': 589.0570146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 8]}]}], 'headings': ['A.1.2 SciFact'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='A.1.2 SciFact\\nPlease write a scientific paper passage to support/refute the claim\\nClaim: [Claim]\\nPassage:'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/198', 'parent': {'$ref': '#/groups/4'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 540.4740146484376, 'r': 294.384, 'b': 531.1090146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 47]}]}, {'self_ref': '#/texts/199', 'parent': {'$ref': '#/groups/4'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 526.9250146484375, 'r': 181.563, 'b': 517.5600146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 18]}]}, {'self_ref': '#/texts/200', 'parent': {'$ref': '#/groups/4'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 513.3750146484375, 'r': 171.406, 'b': 504.0100146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 17]}]}], 'headings': ['A.1.3 Arguana'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='A.1.3 Arguana\\nPlease write a counter argument for the passage\\nPassage: [PASSAGE]\\nCounter Argument:'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/202', 'parent': {'$ref': '#/groups/5'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 457.51101464843754, 'r': 353.37, 'b': 448.14601464843753, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 62]}]}, {'self_ref': '#/texts/203', 'parent': {'$ref': '#/groups/5'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 443.9620146484375, 'r': 193.955, 'b': 434.5970146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 20]}]}, {'self_ref': '#/texts/204', 'parent': {'$ref': '#/groups/5'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 430.4120146484375, 'r': 123.864, 'b': 421.04701464843754, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 8]}]}], 'headings': ['A.1.4 TREC-COVID'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='A.1.4 TREC-COVID\\nPlease write a scientific paper passage to answer the question\\nQuestion: [QUESTION]\\nPassage:'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/206', 'parent': {'$ref': '#/groups/6'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 372.7800146484375, 'r': 354.581, 'b': 363.41501464843753, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 63]}]}, {'self_ref': '#/texts/207', 'parent': {'$ref': '#/groups/6'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 359.2310146484375, 'r': 193.955, 'b': 349.8660146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 20]}]}, {'self_ref': '#/texts/208', 'parent': {'$ref': '#/groups/6'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 345.68201464843753, 'r': 123.864, 'b': 336.3170146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 8]}]}], 'headings': ['A.1.5 FiQA'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='A.1.5 FiQA\\nPlease write a financial article passage to answer the question\\nQuestion: [QUESTION]\\nPassage:'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/210', 'parent': {'$ref': '#/groups/7'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 287.73401464843755, 'r': 287.021, 'b': 278.36901464843754, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 46]}]}, {'self_ref': '#/texts/211', 'parent': {'$ref': '#/groups/7'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 274.1850146484376, 'r': 193.955, 'b': 264.82001464843756, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 20]}]}, {'self_ref': '#/texts/212', 'parent': {'$ref': '#/groups/7'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 260.6350146484375, 'r': 123.864, 'b': 251.2700146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 8]}]}], 'headings': ['A.1.6 DBPedia-Entity'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='A.1.6 DBPedia-Entity\\nPlease write a passage to answer the question.\\nQuestion: [QUESTION]\\nPassage:'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/214', 'parent': {'$ref': '#/groups/8'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 204.77101464843747, 'r': 278.872, 'b': 195.40601464843758, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 44]}]}, {'self_ref': '#/texts/215', 'parent': {'$ref': '#/groups/8'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.118, 't': 191.2220146484375, 'r': 155.696, 'b': 181.85701464843748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 14]}]}, {'self_ref': '#/texts/216', 'parent': {'$ref': '#/groups/8'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 177.67201464843754, 'r': 123.864, 'b': 168.30701464843753, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 8]}]}], 'headings': ['A.1.7 TREC-NEWS'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='A.1.7 TREC-NEWS\\nPlease write a news passage about the topic.\\nTopic: [TOPIC]\\nPassage:'), Document(metadata={'source': 'https://arxiv.org/pdf/2212.10496', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/218', 'parent': {'$ref': '#/groups/9'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 119.72401464843756, 'r': 485.85, 'b': 110.35901464843755, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 91]}]}, {'self_ref': '#/texts/219', 'parent': {'$ref': '#/groups/9'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 106.17501464843747, 'r': 193.955, 'b': 96.81001464843757, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 20]}]}, {'self_ref': '#/texts/220', 'parent': {'$ref': '#/groups/9'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 11, 'bbox': {'l': 86.457, 't': 92.62601464843749, 'r': 123.864, 'b': 83.26101464843748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 8]}]}], 'headings': ['A.1.8 Mr.TyDi'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 16736323509490631985, 'filename': '2212.10496v1.pdf'}}}, page_content='A.1.8 Mr.TyDi\\nPlease write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail.\\nQuestion: [QUESTION]\\nPassage:')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng8isqI8Zp-S",
        "outputId": "dece508b-2949-457d-90fc-f1c4d6703049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- d.page_content='Precise Zero-Shot Dense Retrieval without Relevance Labels\\nLuyu Gao âˆ— â€  Xueguang Ma âˆ— â€¡ Jimmy Lin â€¡ Jamie Callan â€ \\nâ€ \\nLanguage Technologies Institute, Carnegie Mellon University â€¡ David R. Cheriton School of Computer Science, University of Waterloo {luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca'\n",
            "- d.page_content=\"Abstract\\nWhile dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings ( HyDE ). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT ) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder (e.g. Contriever ) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search,\"\n",
            "- d.page_content='Abstract\\nQA, fact verification) and languages (e.g. sw, ko, ja). 1'\n",
            "- d.page_content='1 Introduction\\nDense retrieval (Lee et al., 2019; Karpukhin et al., 2020), the method of retrieving documents using semantic embedding similarities, has been shown successful across tasks like web search, question answering, and fact verification. A variety of methods such as negative mining (Xiong et al., 2021; Qu et al., 2021), distillation (Qu et al., 2021; Lin et al., 2021b; HofstÃ¤tter et al., 2021) and task-specific\\nâˆ— Equal contribution.\\n1 No models were trained or fine-tuned in making this preprint. Our open source code is available at https://github. com/texttron/hyde .\\npre-training (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) have been proposed to improve the effectiveness of supervised dense retrieval models.'\n",
            "- d.page_content='1 Introduction\\nOn the other hand, zero-shot dense retrieval still remains difficult. Many recent works consider the alternative transfer learning setup, where the dense retrievers are trained on a high-resource dataset and then evaluated on queries from new tasks. The MSMARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged querydocument pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in practice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO restricts commercial use and cannot be adopted in a variety of real-world search scenarios.'\n",
            "- d.page_content='1 Introduction\\nIn this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and generalize across tasks. As supervision is not available, we start by examining self-supervised representation learning methods. Modern deep learning enables two distinct learning algorithms. At the token level, generative large language models (LLM) pretrained on large corpus have demonstrated strong natural language understanding (NLU) and generation (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse unseen instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et'\n",
            "- d.page_content='1 Introduction\\nal., 2022; Wei et al., 2022). Ouyang et al. (2022) show that with a small amount of data, GPT-3 (Brown et al., 2020) models can be aligned\\nFigure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries without changing the underlying GPT-3 and Contriever / mContriever models.\\nto human intent to follow instructions.'\n",
            "- d.page_content='1 Introduction\\nWith these ingredients, we propose to pivot through Hypothetical Document Embeddings ( HyDE ), and decompose dense retrieval into two tasks, a generative task performed by an instruction-following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to \"write a document that answers the question\", i.e. a hypothetical document. We expect the generative process to capture \"relevance\" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder\\'s dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner-product during contrastive training. Note that, interestingly, with HyDE factorization, the'\n",
            "- d.page_content='1 Introduction\\nquery-document similarity score is no longer explicitly modeled nor computed. Instead, the retrieval task is cast into two NLU and NLG tasks.\\nHyDE appears unsupervised. No model is trained in HyDE : both the generative model and the contrastive encoder remain intact. Supervision signals were only involved in instruction learning of our backbone LLM.\\nIn our experiments, we show HyDE using InstructGPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) as backbone models significantly outperforms the previous state-of-the-art Contrieveronly zero-shot no-relevance system on 11 queries sets, covering tasks like Web Search, Question Answering, Fact Verification and languages like Swahili, Korean, Japanese.'\n",
            "- d.page_content='2 Related Works\\nDense Retrieval (Lee et al., 2019; Karpukhin et al., 2020) has been extensively studied after the emergence of pre-trained Transformer language models (Devlin et al., 2019). Researchers studied the metric learning problems, such as training loss (Karpukhin et al., 2020) and negative sampling (Xiong et al., 2021; Qu et al., 2021), and also introduced distillation (Qu et al., 2021; Lin et al., 2021b; HofstÃ¤tter et al., 2021). Later works studied the second stage pre-training of language model specifically for retrieval (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022).\\nThe popularity of dense retrieval can be partially attributed to the rich and successful research in very efficient minimum inner product search (MIPS) at very large (billion) scales (Johnson et al., 2017).'\n",
            "- d.page_content=\"2 Related Works\\nInstructions-Following Language Models Soon after the emergence of LLMs, several groups of researchers discover that LLMs trained on data consisting of instructions and their execution can zero-shot generalize to perform new tasks with new instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). This can be done by standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022).\\nConcurrent to us, Asai et al. (2022) studied 'Task-aware Retrieval with Instructions'. They fine-tuned dense encoders that can also encode task-specific instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an instruction following generative LLM, as described above.\"\n",
            "- d.page_content='2 Related Works\\nZero-Shot Dense Retrieval The tasks of zeroshot (dense) retrieval are arguably empirically defined by Thakur et al. (2021) for the neural retrieval community. Their BEIR benchmark consists of diverse retrieval tasks. The paper and many follow-up research generally consider the Transfer Learning setup where the dense retriever is first learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022).\\nHowever, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora.'\n",
            "- d.page_content=\"2 Related Works\\nBy the definition in Sachan et al. (2022), our setup can be roughly considered as 'unsupervised' . Strictly, as with Sachan et al. (2022), the only supervision resides in the LLM, in the processing of learning to follow instructions.\\nGenerative Retrieval Generative search is a new class of retrieval methods that use neural generative models as search indices (Metzler et al., 2021; Tay et al., 2022; Bevilacqua et al., 2022; Lee et al., 2022). These models use (constrained) decoding to generate document identifiers, such as id and sub-string, which map directly to real documents. They have to go through special training procedures over relevance data; effective search may also need to use novel forms of search indices (Bevilacqua et al., 2022; Lee et al., 2022). In comparison, our method uses the standard MIPS index and requires no training or training data. Our generative model produces an intermediate hypothetical document to be fed into a dense encoder, instead of a real document.\"\n",
            "- d.page_content='3 Methodology\\nIn this section, we first formally define the problem of (zero-shot) dense retrieval. Then we will introduce how HyDE is designed to solve it.'\n",
            "- d.page_content='3.1 Preliminaries\\nDense retrieval models similarity between query and document with inner product similarity. Given a query q and document d , it uses two encoder function enc q and enc d to map them into d dimension vectors v q , v d , whose inner product is used as similarity measurement.\\n<!-- formula-not-decoded -->\\nFor zero-shot retrieval, we consider L query sets Q 1 , Q 2 , ..., Q L and their corresponding search corpus, document sets D 1 , D 2 , ..., D L . Denote the j -th query from i -th set query set Q i as q ij . We need to fully define mapping functions enc q and enc d without access to any query set Q i , document set D i , or any relevance judgment r ij .\\nThe difficulty of zero-shot dense retrieval lies precisely in Equation 1: it requires learning of two embedding functions (for query and document respectively) into the same embedding space where inner product captures relevance . Without relevance judgments/scores to fit, learning becomes intractable.'\n",
            "- d.page_content='3.2 HyDE\\nHyDE circumvents the aforementioned learning problem by performing search in documentonly embedding space that captures documentdocument similarity. This can be easily learned using unsupervised contrastive learning (Izacard et al., 2021; Gao et al., 2021; Gao and Callan, 2022). We set document encoder enc d directly as a contrastive encoder enccon.\\n<!-- formula-not-decoded -->\\nThis function is also denoted as f for simplicity. This unsupervised contrastive encoder will be shared by all incoming document corpus.\\n<!-- formula-not-decoded -->\\nTo build the query vector, we consider in addition an instruction following LM, InstructLM. It takes a query q and a textual instruction INST and follows them to perform the task specified by INST. For simplicity, denote,\\n<!-- formula-not-decoded -->\\nNow we can use g to map queries to \"hypothetical\" documents by sampling from g , setting INST'\n",
            "- d.page_content=\"3.2 HyDE\\nto be 'write a paragraph that answers the question' . The generated document is not real, can and is likely to be ungrounded factually (Brown et al., 2020; Thoppilan et al., 2022). We only require it to capture relevance pattern. This is done by generating documents, i.e. providing examples. Critically, here we offload relevance modeling from representation learning model to an NLG model that generalizes significantly more easily, naturally, and effectively (Brown et al., 2020; Ouyang et al., 2022). Generating examples also replaces explicit modeling of relevance scores.\\nWe can now encode the generated document using the document encoder f . Write,\\n<!-- formula-not-decoded -->\\nFormally, g defines a probability distribution based on the chain rule. In this paper, we simply consider the expectation value, assuming the distribution of v q ij is uni-modal, i.e. the query is not ambiguous. The study of ambiguous queries and diversity is left to future work. We estimate Equation 5 by sampling N documents from g , [ Ë† d 1 , Ë† d 2 , ..., Ë† d N ] .\"\n",
            "- d.page_content='3.2 HyDE\\n<!-- formula-not-decoded -->\\n<!-- formula-not-decoded -->\\nWealso consider the query as a possible hypothesis,\\n<!-- formula-not-decoded -->\\nInner product is computed between Ë† v q ij and the set of all document vectors { f ( d ) | d âˆˆ D i } . The most similar documents are retrieved. Here the encoder function f serves as a lossy compressor that outputs dense vectors, where the extra details are filtered and left out from the vector. It further grounds the hypothetical vector to the actual corpus and the real documents. The full HyDE system is illustrated in Figure 1.'\n",
            "- d.page_content='4.1 Setup\\nImplementation We implement HyDE using InstructGPT , a GPT-3 model from the instruct series ( text-davinci-003 ; Ouyang et al. (2022)) and Contriever models (Izacard et al., 2021). We sample from InstructGPT using the OpenAI playground default temperature of 0.7 for open-ended generations. We use the English-only Contriever model for English retrieval tasks and multilingual mContriever for non-English tasks. We conducted retrieval experiments with the Pyserini toolkit (Lin et al., 2021a).'\n",
            "- d.page_content='4.1 Setup\\nDatasets We consider web search query sets TREC DL19 (Craswell et al., 2020a) and DL20 (Craswell et al., 2020b); they are based on the MS-MARCO dataset (Bajaj et al., 2016). We also use a diverse collection of 6 low-resource datasets from the BEIR dataset (Thakur et al., 2021). For non-English retrieval, we consider Swahili, Korean, Japanese, and Bengali from the Mr.Tydi dataset (Zhang et al., 2021).\\nWe use different instructions for each dataset. They share a similar structure but have different quantifiers to control the exact form of the generated hypothetical documents. These instructions can be found in subsection A.1.'\n",
            "- d.page_content='4.1 Setup\\nCompared Systems Contriever models, Contriever and mContriever , serve as our major baseline. They are trained using unsupervised contrastive learning. HyDE retrievers share the exact same embedding spaces with them. The only difference is how the query vector is built. These comparisons allow us to easily examine the effect of HyDE . The classical heuristic-based lexical retriever BM25 is also included.'\n",
            "- d.page_content='4.1 Setup\\nSeveral systems that involve fine-tuning on massive relevance data are also included as references. We consider models fine-tuned on MSMARCO and transferred, DPR and ANCE, from the BEIR paper. For multilingual, we include the mDPR model from Mr.Tydi paper and MSMARCO fine-tuned mBERT and XLM-R from the Contriever paper. We also include the state-ofthe-art transfer learning models: Contriever and mContriever fine-tuned on MS-MARCO, denoted Contriever FT and mContriever FT . These models have run through the state-of-the-art retrieval model training pipeline that involves second-stage retrieval-specific pre-training (Lee et al., 2019) and a few rounds of fine-tuning (Qu et al., 2021); they should be considered empirical upper bounds.'\n",
            "- d.page_content='4.2 Web Search\\nIn Table 1, we show retrieval results on TREC DL19 and TREC DL20. We see HyDE bring sizable improvements to Contriever across the board for\\n\\nTable 1: Results for web search on DL19/20. Best performing w/o relevance and overall system(s) are marked bold . DPR, ANCE and Contriever FT are in-domain supervised models that are finetuned on MS MARCO training data.'\n",
            "- d.page_content='4.2 Web Search\\nw/o relevance judgement, DL19.map = w/o relevance judgement. w/o relevance judgement, DL19.ndcg@10 = w/o relevance judgement. w/o relevance judgement, DL19.recall@1k = . w/o relevance judgement, DL20.map = . w/o relevance judgement, DL20.ndcg@10 = . w/o relevance judgement, DL20.recall@1k = . BM25, DL19.map = 30.1. BM25, DL19.ndcg@10 = 50.6. BM25, DL19.recall@1k = 75.0. BM25, DL20.map = 28.6. BM25, DL20.ndcg@10 = 48.0. BM25, DL20.recall@1k = 78.6. Contriever, DL19.map = 24.0. Contriever, DL19.ndcg@10 = 44.5. Contriever, DL19.recall@1k = 74.6. Contriever, DL20.map = 24.0. Contriever,'\n",
            "- d.page_content='4.2 Web Search\\nDL20.ndcg@10 = 42.1. Contriever, DL20.recall@1k = 75.4. HyDE, DL19.map = 41.8. HyDE, DL19.ndcg@10 = 61.3. HyDE, DL19.recall@1k = 88.0. HyDE, DL20.map = 38.2. HyDE, DL20.ndcg@10 = 57.9. HyDE, DL20.recall@1k = 84.4. w/ relevance judgement, DL19.map = w/ relevance judgement. w/ relevance judgement, DL19.ndcg@10 = w/ relevance judgement. w/ relevance judgement, DL19.recall@1k = . w/ relevance judgement, DL20.map = . w/ relevance judgement, DL20.ndcg@10 = . w/ relevance judgement, DL20.recall@1k = . DPR, DL19.map = 36.5. DPR, DL19.ndcg@10 = 62.2. DPR, DL19.recall@1k = 76.9. DPR, DL20.map ='\n",
            "- d.page_content='4.2 Web Search\\n41.8. DPR, DL20.ndcg@10 = 65.3. DPR, DL20.recall@1k = 81.4. ANCE, DL19.map = 37.1. ANCE, DL19.ndcg@10 = 64.5. ANCE, DL19.recall@1k = 75.5. ANCE, DL20.map = 40.8. ANCE, DL20.ndcg@10 = 64.6. ANCE, DL20.recall@1k = 77.6. Contriever FT, DL19.map = 41.7. Contriever FT, DL19.ndcg@10 = 62.1. Contriever FT, DL19.recall@1k = 83.6. Contriever FT, DL20.map = 43.6. Contriever FT, DL20.ndcg@10 = 63.2. Contriever FT, DL20.recall@1k = 85.8\\n\\nTable 2: Low resource tasks from BEIR. Best performing w/o relevance and overall system(s) are marked bold .'\n",
            "- d.page_content='4.2 Web Search\\nnDCG@10, Scifact = nDCG@10. nDCG@10, Arguana = nDCG@10. nDCG@10, Trec-Covid = nDCG@10. nDCG@10, FiQA = nDCG@10. nDCG@10, DBPedia = nDCG@10. nDCG@10, TREC-NEWS = nDCG@10. w/o relevance judgement, Scifact = w/o relevance judgement. w/o relevance judgement, Arguana = w/o relevance judgement. w/o relevance judgement, Trec-Covid = w/o relevance judgement. w/o relevance judgement, FiQA = w/o relevance judgement. w/o relevance judgement, DBPedia = w/o relevance judgement. w/o relevance judgement, TREC-NEWS = w/o relevance judgement. BM25, Scifact = 67.9. BM25, Arguana = 39.7. BM25, Trec-Covid = 59.5. BM25, FiQA = 23.6. BM25,'\n",
            "- d.page_content='4.2 Web Search\\nDBPedia = 31.8. BM25, TREC-NEWS = 39.5. Contriever, Scifact = 64.9. Contriever, Arguana = 37.9. Contriever, Trec-Covid = 27.3. Contriever, FiQA = 24.5. Contriever, DBPedia = 29.2. Contriever, TREC-NEWS = 34.8. HyDE, Scifact = 69.1. HyDE, Arguana = 46.6. HyDE, Trec-Covid = 59.3. HyDE, FiQA = 27.3. HyDE, DBPedia = 36.8. HyDE, TREC-NEWS = 44.0. w/ relevance judgement, Scifact = w/ relevance judgement. w/ relevance judgement, Arguana = w/ relevance judgement. w/ relevance judgement, Trec-Covid = w/ relevance judgement. w/ relevance judgement, FiQA = w/ relevance judgement. w/ relevance judgement, DBPedia = w/ relevance judgement. w/ relevance judgement, TREC-NEWS = w/ relevance judgement. DPR, Scifact ='\n",
            "- d.page_content='4.2 Web Search\\n31.8. DPR, Arguana = 17.5. DPR, Trec-Covid = 33.2. DPR, FiQA = 29.5. DPR, DBPedia = 26.3. DPR, TREC-NEWS = 16.1. ANCE, Scifact = 50.7. ANCE, Arguana = 41.5. ANCE, Trec-Covid = 65.4. ANCE, FiQA = 30.0. ANCE, DBPedia = 28.1. ANCE, TREC-NEWS = 38.2. Contriever FT, Scifact = 67.7. Contriever FT, Arguana = 44.6. Contriever FT, Trec-Covid = 59.6. Contriever FT, FiQA = 32.9. Contriever FT, DBPedia = 41.3. Contriever FT, TREC-NEWS = 42.8. Recall@100, Scifact = Recall@100. Recall@100, Arguana = Recall@100. Recall@100, Trec-Covid = Recall@100. Recall@100,'\n",
            "- d.page_content='4.2 Web Search\\nFiQA = Recall@100. Recall@100, DBPedia = Recall@100. Recall@100, TREC-NEWS = Recall@100. w/o relevance judgement, Scifact = w/o relevance judgement. w/o relevance judgement, Arguana = w/o relevance judgement. w/o relevance judgement, Trec-Covid = w/o relevance judgement. w/o relevance judgement, FiQA = w/o relevance judgement. w/o relevance judgement, DBPedia = w/o relevance judgement. w/o relevance judgement, TREC-NEWS = w/o relevance judgement. BM25, Scifact = 92.5. BM25, Arguana = 93.2. BM25, Trec-Covid = 49.8. BM25, FiQA = 54.0. BM25, DBPedia = 46.8. BM25, TREC-NEWS = 44.7. Contriever, Scifact = 92.6. Contriever, Arguana = 90.1. Contriever, Trec-Covid = 17.2. Contriever, FiQA ='\n",
            "- d.page_content='4.2 Web Search\\n56.2. Contriever, DBPedia = 45.3. Contriever, TREC-NEWS = 42.3. HyDE, Scifact = 96.4. HyDE, Arguana = 97.9. HyDE, Trec-Covid = 41.4. HyDE, FiQA = 62.1. HyDE, DBPedia = 47.2. HyDE, TREC-NEWS = 50.9. w/ relevance judgement, Scifact = w/ relevance judgement. w/ relevance judgement, Arguana = w/ relevance judgement. w/ relevance judgement, Trec-Covid = w/ relevance judgement. w/ relevance judgement, FiQA = w/ relevance judgement. w/ relevance judgement, DBPedia = w/ relevance judgement. w/ relevance judgement, TREC-NEWS = w/ relevance judgement. DPR, Scifact = 72.7. DPR, Arguana = 75.1. DPR, Trec-Covid = 21.2. DPR, FiQA = 34.2. DPR, DBPedia = 34.9. DPR, TREC-NEWS = 21.5. ANCE, Scifact'\n",
            "- d.page_content='4.2 Web Search\\n= 81.6. ANCE, Arguana = 93.7. ANCE, Trec-Covid = 45.7. ANCE, FiQA = 58.1. ANCE, DBPedia = 31.9. ANCE, TREC-NEWS = 39.8. Contriever FT, Scifact = 94.7. Contriever FT, Arguana = 97.7. Contriever FT, Trec-Covid = 40.7. Contriever FT, FiQA = 65.6. Contriever FT, DBPedia = 54.1. Contriever FT, TREC-NEWS = 49.2\\nboth precision-oriented and recall metrics. While unsupervised Contriever can underperform the classical BM25 approach, HyDE outperforms BM25 by large margins.'\n",
            "- d.page_content='4.2 Web Search\\nHyDE remains competitive even when compared to fine-tuned models. Note that TREC DL19/20 are search tasks defined on MS-MARCO and there, all the fine-tuned models are richly supervised . On TREC DL19, HyDE shows comparable map and ndcg@10 to Contriever FT and best recall@1k. On DL20, HyDE gets around 10% lower map and ndcg@10 than Contriever FT and similar recall@1k. The ANCE model shows better ndcg@10 numbers than HyDE but lower recall, suggesting it may be biased to a subset of queries and/or relevant documents.'\n",
            "- d.page_content='4.3 Low Resource Retrieval\\nIn Table 2, we show retrieval results on lowresource tasks from BEIR. Similar to web search, HyDE again brings sizable improvements to Contriever across the board in terms of both ndcg and recall. HyDE is only outperformed by BM25 on one dataset, TREC-Covid but with a tiny 0.2 margin; in comparison, the underlying Contriever underperforms by more than 50%.\\nWe also observe HyDE demonstrates strong performance compared to fine-tuned models. HyDE generally shows better performance than ANCE and DPR, even though the two are fine-tuned on MS-MARCO and ANCE also involves some sophisticated hard negative techniques. Contriever FT shows performance advantages on FiQA and DBPedia. These involve retrieval of financial posts or entities respectively. We believe the performance difference can be attributed to the\\n\\nTable 3: MRR@100 on Mr.Tydi. Best performing w/o relevance and overall system(s) are marked bold .'\n",
            "- d.page_content='4.3 Low Resource Retrieval\\nw/o relevance judgement, Swahili = w/o relevance judgement. w/o relevance judgement, Korean = . w/o relevance judgement, Japanese = . w/o relevance judgement, Bengali = . BM25, Swahili = 38.9. BM25, Korean = 28.5. BM25, Japanese = 21.2. BM25, Bengali = 41.8. mContriever, Swahili = 38.3. mContriever, Korean = 22.3. mContriever, Japanese = 19.5. mContriever, Bengali = 35.3. HyDE, Swahili = 41.7. HyDE, Korean = 30.6. HyDE, Japanese = 30.7. HyDE, Bengali = 41.3. w/ relevance judgement, Swahili = w/ relevance judgement. w/ relevance judgement, Korean = . w/ relevance judgement, Japanese = . w/ relevance judgement, Bengali = . mDPR, Swahili = 7.3. mDPR, Korean = 21.9. mDPR, Japanese = 18.1. mDPR, Bengali = 25.8. mBERT, Swahili ='\n",
            "- d.page_content='4.3 Low Resource Retrieval\\n37.4. mBERT, Korean = 28.1. mBERT, Japanese = 27.1. mBERT, Bengali = 35.1. XLM-R, Swahili = 35.1. XLM-R, Korean = 32.2. XLM-R, Japanese = 24.8. XLM-R, Bengali = 41.7. mContriever FT, Swahili = 51.2. mContriever FT, Korean = 34.2. mContriever FT, Japanese = 32.4. mContriever FT, Bengali = 42.3\\nunder-specification of the instruction; more elaborative instructions may help.'\n",
            "- d.page_content='4.4 Multilingual Retrieval\\nMultilingual setup poses several additional challenges to HyDE . The small-sized contrastive encoder gets saturated as the number of languages scales (Conneau et al., 2020; Izacard et al., 2021). Meanwhile, our generative LLM faces an opposite issue: with languages of not as high resource as English or French, the high capacity LLM can get under-trained (Hoffmann et al., 2022).\\nNevertheless, in Table 3, we still find HyDE able to improve the mContriever model. It can outperform non-Contriever models fine-tuned on and transferred from MS-MARCO. On the other hand, we do observe some margins between HyDE and fine-tuned mContriever FT . Since HyDE and mContriever FT use similar contrastive encoders, we hypothesize this is because the non-English languages we considered are under-trained in both pre-training and instruction learning stages.'\n",
            "- d.page_content='5 Analysis\\nThe generative LLM and contrastive encoder make up the backbone of HyDE . In this section, we study the effect of changing their realizations. In particular, we consider smaller language models (LM) and fine-tuned encoders. We conduct our studies on TREC DL19/20.'\n",
            "- d.page_content='5.1 Effect of Different Generative Models\\nIn Table 4, we show HyDE using other instruction-following language models. In particular, we consider a 52-billion Cohere model ( command-xlarge-20221108 ) and a 11-billion FLAN model ( FLAN-T5-xxl ; Wei et al. (2022)). 2 Generally, we observe that all\\n2 Model sizes are from https://crfm.stanford.edu/ helm/v1.0/?models .\\n\\nTable 4: NDCG@10 on TREC DL19/20. Effect of changing different instruction LMs and using finetuned encoder. Best w/o relevance and overall models are marked bold .'\n",
            "- d.page_content='5.1 Effect of Different Generative Models\\nContriever, DL19 = 44.5. Contriever, DL20 = 42.1. Contriever FT, DL19 = 62.1. Contriever FT, DL20 = 63.2. HyDE, DL19 = . HyDE, DL20 = . w/ Contriever, DL19 = . w/ Contriever, DL20 = . w/ Flan-T5 (11b), DL19 = 48.9. w/ Flan-T5 (11b), DL20 = 52.9. w/ Cohere (52b), DL19 = 53.8. w/ Cohere (52b), DL20 = 53.8. w/ GPT (175b), DL19 = 61.3. w/ GPT (175b), DL20 = 57.9. w/ Contriever FT, DL19 = . w/ Contriever FT, DL20 = . w/ Flan-T5 (11b), DL19 = 60.2. w/ Flan-T5 (11b), DL20 = 62.1. w/ Cohere'\n",
            "- d.page_content='5.1 Effect of Different Generative Models\\n(52b), DL19 = 61.4. w/ Cohere (52b), DL20 = 63.1. w/ GPT (175b), DL19 = 67.4. w/ GPT (175b), DL20 = 63.5\\nmodels bring improvement to the unsupervised Contriever , with larger models bringing larger improvements. At the time when this paper is written, the Cohere model is still experimental without much detail disclosed. We can only tentatively hypothesize that training techniques may have also played some role in the performance difference.'\n",
            "- d.page_content='5.2 HyDE with Fine-tuned Encoder\\nTo begin with, HyDE with fine-tuned encoder is not the intended usage: HyDE is more powerful and irreplaceable when few relevance labels are present. Here we are interested to find out if and how HyDE embedding can affect fine-tuned encoders. In Table 4, we see that less powerful instruction LMs can negatively impact the overall performance of the fine-tuned retriever. (To remind our readers, Contriever FT is in-domain supervisedly fine-tuned for TREC DL19/20). The performance degradations remain small. On the other hand, we also observe the InstructGPT model able to further bring up the performance, especially on DL19. This suggests that there may still exist certain factors not captured by the fine-tuned encoder but only by the generative model.'\n",
            "- d.page_content='6 Conclusion\\nAt the end of the paper, we encourage the readers to take a moment and reflect on the HyDE model. Compare it to some of the other recently seen retrievers or re-ranker. These other models probably differ in their architecture, training method, and/or task, but probably all of them involve modeling relevance scores between a pair of query and docu- ment. Dense retrievers consider vector similarities while self-attentive re-rankers regression scores. In comparison, the concept of relevance in HyDE is captured by an NLG model and the language generation process. We demonstrate in many cases, HyDE can be as effective as dense retrievers that learn to model numerical relevance scores. So, is numerical relevance just a statistical artifact of language understanding? Will a weak retriever theoretically suffice as the NLU & NLG models rapidly become stronger? Rushing to conclusions is not smart; more works need to be done to get answers. With this paper, we just want to raise these questions.'\n",
            "- d.page_content='6 Conclusion\\nConcretely in this paper, we introduce a new paradigm of interactions between LLM and dense encoder/retriever. We demonstrate (part of) relevance modeling and instruction understanding can be delegated to the more powerful and flexible LLM. As a consequence, the need for relevance labels is removed. We are excited to see how this can be generalized further to more sophisticated tasks like multi-hop retrieval/QA and conversational search.\\nWeargue HyDE is also of practical use though not necessarily over the entire lifespan of a search system. At the very beginning of the life of the search system, serving queries using HyDE offers performance comparable to a fine-tuned model, which no other relevance-free model can offer. As the search log grows, a supervised dense retriever can be gradually rolled out. As the dense retriever grows stronger, more queries will be routed to it, with only less common and emerging ones going to HyDE backend.'\n",
            "- d.page_content='References\\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. Task-aware retrieval with instructions.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2016. Ms marco: A human generated machine reading comprehension dataset.\\nMichele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive search engines: Generating substrings as document identifiers. CoRR , abs/2204.10628.'\n",
            "- d.page_content='References\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .'\n",
            "- d.page_content='References\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and'\n",
            "- d.page_content='References\\nWojciech Zaremba. 2021. Evaluating large language models trained on code.'\n",
            "- d.page_content='References\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child,'\n",
            "- d.page_content='References\\nOleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettle-'\n",
            "- d.page_content='References\\n- moyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 84408451, Online. Association for Computational Linguistics.\\n- Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2020a. Overview of the trec 2019 deep learning track.\\n- Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. Voorhees. 2020b. Overview of the trec 2020 deep learning track. ArXiv , abs/2003.07820.\\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.'\n",
            "- d.page_content='References\\n- Luyu Gao and Jamie Callan. 2021. Condenser: a pretraining architecture for dense retrieval. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 981-993, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n- Luyu Gao and Jamie Callan. 2022. Unsupervised corpus aware language model pre-training for dense passage retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2843-2853, Dublin, Ireland. Association for Computational Linguistics.\\n- Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.'\n",
            "- d.page_content=\"References\\n- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models.\\n- Sebastian HofstÃ¤tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval ,\\n- SIGIR '21, page 113-122, New York, NY, USA. Association for Computing Machinery.\"\n",
            "- d.page_content='References\\n- Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. CoRR , abs/2112.09118.\\n- Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2017. Billion-scale similarity search with gpus. CoRR , abs/1702.08734.\\n- Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 67696781, Online. Association for Computational Linguistics.\\n- Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. 2022. Generative multi-hop retrieval.'\n",
            "- d.page_content='References\\n- Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6086-6096, Florence, Italy. Association for Computational Linguistics.\\n- Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021a. Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021) , pages 2356-2362.\\n- Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021b. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021) , pages 163173, Online. Association for Computational Linguistics.'\n",
            "- d.page_content='References\\n- Zheng Liu and Yingxia Shao. 2022. Retromae: Pretraining retrieval-oriented transformers via masked auto-encoder. ArXiv , abs/2205.12035.\\n- Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold Overwijk. 2021. Less is more: Pretrain a strong Siamese encoder for dense text retrieval using a weak decoder. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 2780-2791, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n- Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 2021. Rethinking search: making domain experts out of dilettantes. SIGIR Forum , 55(1):13:1-13:27.'\n",
            "- d.page_content='References\\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2791-2809, Seattle, United States. Association for Computational Linguistics.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.'\n",
            "- d.page_content='References\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 5835-5847, Online. Association for Computational Linguistics.'\n",
            "- d.page_content='References\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen,'\n",
            "- d.page_content=\"References\\nZhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis & insights from training gopher.\\nDevendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage retrieval with zero-shot question generation.\"\n",
            "- d.page_content='References\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault FÃ©vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net.'\n",
            "- d.page_content='References\\nYi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Prakash Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022. Transformer memory as a differentiable search index. CoRR , abs/2202.06991.\\nNandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. CoRR , abs/2104.08663.'\n",
            "- d.page_content='References\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil,'\n",
            "- d.page_content='References\\nBlaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications. CoRR , abs/2201.08239.\\nKexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022. GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2345-2360, Seattle, United States. Association for Computational Linguistics.\\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net.'\n",
            "- d.page_content='References\\n- Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net.\\n- Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and Arnold Overwijk. 2022. Coco-dr: Combating distribution shifts in zero-shot dense retrieval with contrastive and distributionally robust learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing .\\n- Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021. Mr. TyDi: A multi-lingual benchmark for dense retrieval. arXiv:2108.08787 .'\n",
            "- d.page_content='A.1.1 Web Search\\nPlease write a passage to answer the question\\nQuestion: [QUESTION]\\nPassage:'\n",
            "- d.page_content='A.1.2 SciFact\\nPlease write a scientific paper passage to support/refute the claim\\nClaim: [Claim]\\nPassage:'\n",
            "- d.page_content='A.1.3 Arguana\\nPlease write a counter argument for the passage\\nPassage: [PASSAGE]\\nCounter Argument:'\n",
            "- d.page_content='A.1.4 TREC-COVID\\nPlease write a scientific paper passage to answer the question\\nQuestion: [QUESTION]\\nPassage:'\n",
            "- d.page_content='A.1.5 FiQA\\nPlease write a financial article passage to answer the question\\nQuestion: [QUESTION]\\nPassage:'\n",
            "- d.page_content='A.1.6 DBPedia-Entity\\nPlease write a passage to answer the question.\\nQuestion: [QUESTION]\\nPassage:'\n",
            "- d.page_content='A.1.7 TREC-NEWS\\nPlease write a news passage about the topic.\\nTopic: [TOPIC]\\nPassage:'\n",
            "- d.page_content='A.1.8 Mr.TyDi\\nPlease write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail.\\nQuestion: [QUESTION]\\nPassage:'\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "for d in splits[:]:\n",
        "    print(f\"- {d.page_content=}\")\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5KcJlfoZp-X"
      },
      "source": [
        "## Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install milvus_lite langchain_classic"
      ],
      "metadata": {
        "id": "8EzjasocrrQa",
        "outputId": "eecdf1b4-8ff3-40d7-a7af-00c6f421fb4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting milvus_lite\n",
            "  Downloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl.metadata (10.0 kB)\n",
            "Collecting langchain_classic\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from milvus_lite) (4.67.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_classic) (1.2.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain_classic)\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain_classic) (0.4.58)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_classic) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_classic) (6.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_classic) (2.32.4)\n",
            "Requirement already satisfied: sqlalchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_classic) (2.0.45)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (25.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_classic) (0.12.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.17->langchain_classic) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->langchain_classic) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->langchain_classic) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->langchain_classic) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->langchain_classic) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3.0.0,>=1.4.0->langchain_classic) (3.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain_classic) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_classic) (3.0.0)\n",
            "Downloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl (55.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.3/55.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: milvus_lite, langchain-text-splitters, langchain_classic\n",
            "Successfully installed langchain-text-splitters-1.1.0 langchain_classic-1.0.0 milvus_lite-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "1qsKnWLQZp-X"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_milvus import Milvus\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n",
        "\n",
        "\n",
        "milvus_uri = str(Path(mkdtemp()) / \"docling.db\")  # or set as needed\n",
        "vectorstore = Milvus.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    collection_name=\"docling_demo\",\n",
        "    connection_args={\"uri\": milvus_uri},\n",
        "    index_params={\"index_type\": \"FLAT\"},\n",
        "    drop_old=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I54yxhyzZp-Y"
      },
      "source": [
        "## RAG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q openai langchain-openai"
      ],
      "metadata": {
        "id": "lfqqBBENtK_2",
        "outputId": "201620ea-2c10-4fa1-bf23-b3220e1713da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/84.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "ppxvoxI_Zp-Y"
      },
      "outputs": [],
      "source": [
        "# --- Replacement for HuggingFaceEndpoint ---\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "# The same settings you used for LlamaIndex will apply here\n",
        "HUGGINGFACE_OPENAI_BASE_URL = \"https://router.huggingface.co/hf-inference/models/HuggingFaceTB/SmolLM3-3B/v1\"\n",
        "GEN_MODEL_ID_FOR_OPENAI = \"HuggingFaceTB/SmolLM3-3B\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=GEN_MODEL_ID_FOR_OPENAI, # The model name (required by the router)\n",
        "    api_key=HF_TOKEN,             # Your Hugging Face token\n",
        "    base_url=HUGGINGFACE_OPENAI_BASE_URL, # The custom Hugging Face router URL\n",
        ")\n",
        "\n",
        "# --- The rest of your RAG chain remains the same ---\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
        "question_answer_chain = create_stuff_documents_chain(llm, PROMPT) # 'llm' is now ChatOpenAI\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "def clip_text(text, threshold=1000):\n",
        "# ... [rest of your code]\n",
        "    return f\"{text[:threshold]}...\" if len(text) > threshold else text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"What does 2 related works speaks about instruciton following language models?\""
      ],
      "metadata": {
        "id": "nm6U1fr4r4wn"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "AdjwVxCqZp-Z",
        "outputId": "4e05522e-7907-4d44-e712-31bbe765d856"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIStatusError",
          "evalue": "Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-237833099.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion_answer_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_stuff_documents_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPROMPT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrag_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_retrieval_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_answer_chain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresp_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQUESTION\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclipped_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5546\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5547\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5548\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5549\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/passthrough.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     ) -> dict[str, Any]:\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     async def _ainvoke(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   2056\u001b[0m                 output = cast(\n\u001b[1;32m   2057\u001b[0m                     \u001b[0;34m\"Output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m                     context.run(\n\u001b[0m\u001b[1;32m   2059\u001b[0m                         \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/passthrough.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, value, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m         return {\n\u001b[1;32m    492\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             **self.mapper.invoke(\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mpatch_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3866\u001b[0m                 ]\n\u001b[1;32m   3867\u001b[0m                 output = {\n\u001b[0;32m-> 3868\u001b[0;31m                     \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3869\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3870\u001b[0m                 }\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke_step\u001b[0;34m(step, input_, config, key)\u001b[0m\n\u001b[1;32m   3849\u001b[0m             )\n\u001b[1;32m   3850\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3851\u001b[0;31m                 return context.run(\n\u001b[0m\u001b[1;32m   3852\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m                     \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5546\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5547\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5548\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5549\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m             cast(\n\u001b[1;32m    397\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    399\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1116\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 results.append(\n\u001b[0;32m--> 927\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    928\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1222\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraw_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_response\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m         if (\n\u001b[1;32m   1384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_response_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 )\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m                 \u001b[0mraw_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_raw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIStatusError\u001b[0m: Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}"
          ]
        }
      ],
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "resp_dict = rag_chain.invoke({\"input\": QUESTION})\n",
        "\n",
        "clipped_answer = clip_text(resp_dict[\"answer\"],)\n",
        "print(f\"Question:\\n{resp_dict['input']}\\n\\nAnswer:\\n{clipped_answer}\")\n",
        "for i, doc in enumerate(resp_dict[\"context\"]):\n",
        "    print()\n",
        "    print(f\"Source {i + 1}:\")\n",
        "    print(f\"  text: {json.dumps(clip_text(doc.page_content,))}\")\n",
        "    for key in doc.metadata:\n",
        "        if key != \"pk\":\n",
        "            val = doc.metadata.get(key)\n",
        "            clipped_val = clip_text(val) if isinstance(val, str) else val\n",
        "            print(f\"  {key}: {clipped_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTIFFu5TZp-Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}